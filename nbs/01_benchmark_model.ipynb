{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark Models\n",
    "\n",
    "This notebook implements simple benchmark models to compare against LSTM performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp benchmark_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.basics import patch\n",
    "from pathlib import Path\n",
    "import hvplot.pandas\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| eval: false\n",
    "DATA = Path(\"../../data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| eval: false\n",
    "df = pd.read_csv(\n",
    "    DATA/'data_cumul.csv', \n",
    "    sep=';', \n",
    "    usecols=['time', 'P_mean', 'P_cumul_7j', 'débit_insitu', 'débit_mgb'], \n",
    "    index_col='time',\n",
    "    converters={\"time\": pd.to_datetime}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def normalize(df):\n",
    "    return (df - df.min()) / (df.max() - df.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| eval: false\n",
    "normalize(df).plot()#.hvplot.line()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Select feature and target columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| eval: false\n",
    "x_col, y_col = ['P_cumul_7j','débit_mgb'], ['débit_insitu']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Scale data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| eval: false\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "features_scaler = RobustScaler()\n",
    "features = df[x_col]\n",
    "df[x_col] = features_scaler.fit_transform(features)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| eval: false\n",
    "train_mask = df.index < '2019-01-01'\n",
    "train = df[train_mask]\n",
    "valid = df[~train_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Feature generator\n",
    "> Feature generator that uses a sliding window of n previous timesteps and polynomial features to predict the next value, enabling learning of temporal patterns and non-linear relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class FeatureGenerator:\n",
    "    \"\"\"\n",
    "    Transforms time series data into feature matrices suitable for machine learning models.\n",
    "    Creates lagged features using a sliding window and optionally generates polynomial features\n",
    "    to capture non-linear relationships between variables.\n",
    "    \"\"\"\n",
    "    def __init__(self, context_window: int = 10, target_window: int = 10, degree: int = 1):\n",
    "        self.context_window = context_window\n",
    "        self.target_window = target_window\n",
    "        self.poly_features = PolynomialFeatures(degree=degree)\n",
    "        \n",
    "    def generate(self, df: pd.DataFrame, x_col: list[str], y_col: list[str]) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        X, y = df[x_col], df[y_col]\n",
    "        if 1 < self.poly_features.degree:\n",
    "            X = pd.DataFrame(self.poly_features.fit_transform(X), index=X.index)\n",
    "        X, y = self.generate_sliding_window_features(X, y)\n",
    "        return X, y\n",
    "\n",
    "\n",
    "    def generate_sliding_window_features(self, X: pd.DataFrame, y: pd.DataFrame) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Creates a feature matrix by combining multiple input variables and their lagged values.\n",
    "        For each time step t, takes values from t-window to t for each input variable\n",
    "        and combines them into a single feature vector. The target value is taken at time t.\n",
    "        This allows the model to learn patterns across multiple timesteps.\n",
    "        \"\"\"\n",
    "        features = []\n",
    "        targets = []\n",
    "        \n",
    "        for i in range(len(X) - self.context_window - self.target_window):\n",
    "            row_features = X.iloc[i:i + self.context_window]\n",
    "            features.append(row_features.values.reshape(-1))\n",
    "            row_targets = y.iloc[i + self.context_window: i + self.context_window + (self.target_window + 1)]\n",
    "            targets.append(row_targets.values.reshape(-1))\n",
    "\n",
    "        features = pd.DataFrame(index=X.index[self.context_window:len(X) - self.target_window], data=features)\n",
    "        targets = pd.DataFrame(\n",
    "            index=y.index[self.context_window:len(X) - self.target_window],\n",
    "            data=targets,\n",
    "            columns=[f\"t+{i}\" for i in range(0, self.target_window+1)])\n",
    "        return features, targets\n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| eval: false\n",
    "feature_generator = FeatureGenerator(1, 0)\n",
    "train_x, train_y = feature_generator.generate(train, x_col, y_col)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark model\n",
    "We chose a linear regression model as our benchmark because:\n",
    "1. When combined with our feature generator, it provides a simple yet effective baseline\n",
    "2. The polynomial features allow it to capture non-linear relationships in the data\n",
    "3. The sliding window enables learning of temporal patterns across multiple timesteps\n",
    "4. Linear regression models are highly interpretable and computationally efficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class SimpleRegressionModel:\n",
    "    def __init__(self):\n",
    "        self.model = LinearRegression()\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        self.model.fit(X, y)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        pred = self.model.predict(X)\n",
    "        return pd.DataFrame(pred, index=X.index, columns=[f\"t+{i}\" for i in range(0, pred.shape[1])])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model training and comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| eval: false\n",
    "predictions = []\n",
    "scores = []\n",
    "for degree in range(1, 4):\n",
    "    for window in range(10, 51, 10):\n",
    "        feature_generator = FeatureGenerator(context_window=window, target_window=10, degree=degree)        \n",
    "        train_x, train_y = feature_generator.generate(train, x_col, y_col)\n",
    "        valid_x, valid_y = feature_generator.generate(valid, x_col, y_col)\n",
    "\n",
    "        model = SimpleRegressionModel()\n",
    "        model.fit(train_x, train_y)\n",
    "        \n",
    "        pred = model.predict(valid_x)\n",
    "        pred_df = pred.copy()\n",
    "        pred_df['degree'] = degree\n",
    "        pred_df['window'] = window\n",
    "        pred_df = pred_df.set_index(['degree', 'window'], append=True)\n",
    "        predictions.append(pred_df)\n",
    "\n",
    "predictions_ds = pd.concat(predictions).reorder_levels(['degree', 'window', 'time']).to_xarray()\n",
    "observations = valid[y_col].to_xarray().sel(time=slice(predictions_ds.time.min(), predictions_ds.time.max()))\n",
    "results_ds = predictions_ds.merge(observations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring\n",
    "\n",
    "Since we have generated multiple predictions with different parameters, we will select only the best performing models according to each metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class BenchmarkScores:\n",
    "    \"\"\"\n",
    "    A class to compute and compare different error metrics between predictions and observations.\n",
    "    \n",
    "    This class provides a flexible way to compute multiple error metrics between predicted and observed values.\n",
    "    New metrics can be easily added by implementing them as methods.\n",
    "    The new metrics can then be used by passing their method names as strings to compute_scores().\n",
    "    \n",
    "    Example:\n",
    "\n",
    "    ```python\n",
    "    class CustomBenchmarkScores(BenchmarkScores):\n",
    "        def mse(self, pred, obs):\n",
    "            error = ((pred - obs)**2).mean(dim=\"time\") \n",
    "            error.name = \"mse\"\n",
    "            return error\n",
    "        \n",
    "    # Can then be used as:\n",
    "    scores = compute_scores(pred, obs, metrics=[\"mae\", \"rmse\", \"mse\"])\n",
    "    ```\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        pass\n",
    "\n",
    "    def compute_scores(self, predictions: xr.DataArray, observations: xr.DataArray, metrics: list[str]):\n",
    "        \"\"\"Compute the scores for the predictions and observations.\"\"\"\n",
    "        scores = []\n",
    "        for metric in metrics:\n",
    "            scores.append(getattr(self, metric)(predictions, observations).to_dataset(name=metric))\n",
    "        return xr.merge(scores)\n",
    "    \n",
    "    def mae(self, pred, obs):\n",
    "        error = abs(pred - obs).mean(dim=\"time\")\n",
    "        error.name = \"mae\"\n",
    "        return error\n",
    "\n",
    "    def rmse(self, pred, obs):\n",
    "        error = np.sqrt(((pred - obs)**2).mean(dim=\"time\"))\n",
    "        error.name = \"rmse\"\n",
    "        return error\n",
    "    \n",
    "    def nse(self, pred, obs):\n",
    "        \"\"\"\n",
    "        Calculate Nash-Sutcliffe Efficiency score.\n",
    "        \n",
    "        $NSE = 1 - \\\\frac{\\\\sum(pred - obs)^2}{\\\\sum(obs - \\\\overline{obs})^2}$\n",
    "        \n",
    "        NSE ranges from -inf to 1, with 1 being a perfect match\n",
    "        \"\"\"\n",
    "        numerator = ((pred - obs)**2).sum(dim=\"time\")\n",
    "        denominator = ((obs - obs.mean(dim=\"time\"))**2).sum(dim=\"time\")\n",
    "        error = 1 - (numerator / denominator)\n",
    "        error.name = \"nse\"\n",
    "        return error\n",
    "    \n",
    "    def kge(self, pred, obs):\n",
    "        \"\"\"Calculate Kling-Gupta Efficiency score.\n",
    "        \n",
    "        $KGE = 1 - sqrt((r-1)^2 + (\\alpha-1)^2 + (\\beta-1)^2)$\n",
    "        \n",
    "        where:\n",
    "        - r = Pearson correlation coefficient\n",
    "        - $\\\\alpha = \\\\frac{std(pred)}{std(obs)}$ \n",
    "        - $\\\\beta = \\\\frac{mean(pred)}{mean(obs)}$\n",
    "        \n",
    "        KGE ranges from -inf to 1, with 1 being a perfect match\n",
    "        \"\"\"\n",
    "        # Calculate components\n",
    "        r = xr.corr(pred, obs, dim=\"time\")\n",
    "        alpha = pred.std(dim=\"time\") / obs.std(dim=\"time\")\n",
    "        beta = pred.mean(dim=\"time\") / obs.mean(dim=\"time\")\n",
    "        \n",
    "        # Calculate KGE\n",
    "        error = 1 - np.sqrt((r - 1)**2 + (alpha - 1)**2 + (beta - 1)**2)\n",
    "        error.name = \"kge\"\n",
    "        return error\n",
    "\n",
    "    def find_nbest_scores(\n",
    "            self,\n",
    "            ds: xr.Dataset, # Dataset containing scores as variables\n",
    "            how: dict[str, str], # Dictionary of how to find the best scores for each variable\n",
    "            n: int=2 # Number of best scores to find\n",
    "        ):\n",
    "        \"\"\"Given a dataset containing scores as variables,find the n best scores for each score.\"\"\"\n",
    "        assert all(how in [\"min\", \"max\"] for how in how.values()), \"how must be either 'min' or 'max'\"\n",
    "        df = ds.to_dataframe()\n",
    "        df = df.reorder_levels([\"variable\"] + [level for level in df.index.names if level != \"variable\"])\n",
    "        best_scores_coords = []\n",
    "        for score, how in how.items():\n",
    "            ascending = how == \"min\"\n",
    "            best_variable_scores = self._find_best_scores_by_variable(df[score], n, ascending)\n",
    "            best_scores_coords.extend(best_variable_scores.values.tolist())\n",
    "        best_scores_coords = pd.MultiIndex.from_tuples(best_scores_coords, names=df.index.names)\n",
    "        n_best_scores = df.loc[best_scores_coords]\n",
    "        n_best_scores = n_best_scores.sort_index().drop_duplicates()\n",
    "        return n_best_scores\n",
    "\n",
    "    def _find_best_scores_by_variable(self, serie: pd.Series, n, ascending: bool):\n",
    "        \"\"\"For each unique variable in a multi-indexed Series, find indices of the n best values based on ascending/descending sort.\"\"\"\n",
    "        sorted = serie.sort_values(ascending=ascending)\n",
    "        nbest_values = []\n",
    "        for v in sorted.index.get_level_values(\"variable\").unique():\n",
    "            nbest_values.append(sorted.loc[[v]].head(n))\n",
    "        nbest_values = pd.concat(nbest_values)\n",
    "        return nbest_values.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| eval: false\n",
    "benchmark_scores = BenchmarkScores()\n",
    "scores_ds = benchmark_scores.compute_scores(\n",
    "    predictions_ds.to_array(),\n",
    "    observations[\"débit_insitu\"],\n",
    "    [\"mae\", \"rmse\", \"nse\", \"kge\"])\n",
    "best_scores = benchmark_scores.find_nbest_scores(\n",
    "    scores_ds,\n",
    "    how={\"mae\": \"min\", \"rmse\": \"min\", \"nse\": \"max\", \"kge\": \"max\"},\n",
    "    n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Static plotting\n",
    "\n",
    "Finally we will plot the error so we can visualize the best results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def plot_static_benchmark_scores(\n",
    "        df: pd.DataFrame, # Dataframe with polynomial degree and window as index and mae and mse as columns\n",
    "        figsize: tuple=(8, 7), # Figure size in inches (width, height)\n",
    "        fontsize: int=7, # Font size for annotations\n",
    "        xlim: tuple=(None, None), # Tuple of (min, max) values for x-axis limits\n",
    "        ylim: tuple=(None, None), # Tuple of (min, max) values for y-axis limits\n",
    "        ):\n",
    "    \"\"\"Plot MAE vs MSE scores with degree and window annotations for model comparison\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    \n",
    "    # Get unique time steps - expected format 't+N' where N is 0-10\n",
    "    time_steps = []\n",
    "    for col in df.index.get_level_values(0).unique():\n",
    "        if isinstance(col, str) and col.startswith('t+'):\n",
    "            time_steps.append(col)\n",
    "    time_steps = sorted(time_steps, key=lambda x: int(x.split('+')[1]))\n",
    "    \n",
    "    # Create a colormap\n",
    "    colors = plt.cm.viridis(np.linspace(0, 1, len(time_steps)))\n",
    "    \n",
    "    # Plot each time step with different color\n",
    "    for t, color in zip(time_steps, colors):\n",
    "        mask = df.index.get_level_values(0) == t\n",
    "        df_t = df[mask]\n",
    "        scatter = ax.scatter(df_t['mae'], df_t['rmse'], label=t, color=color)\n",
    "        \n",
    "        # Add annotations for each point\n",
    "        for (_, deg, win), (mae, rmse) in zip(df_t.index, df_t[['mae', 'rmse']].values):\n",
    "            ax.annotate(f'd={deg},w={win}', (mae, rmse), \n",
    "                       xytext=(5, 5), textcoords='offset points', \n",
    "                       fontsize=fontsize, color=color)\n",
    "\n",
    "    ax.set(xlabel='MAE', ylabel='RMSE', \n",
    "           title='MAE vs RMSE for different degrees, windows and prediction horizons')\n",
    "    ax.legend(title='Prediction horizon', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.xlim(*xlim)\n",
    "    plt.ylim(*ylim)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| eval: false\n",
    "plot_static_benchmark_scores(best_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interactive plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def plot_interactive_benchmark_scores(\n",
    "        df: pd.DataFrame, # Dataframe with polynomial degree and window as index and mae and mse as columns\n",
    "        figsize: tuple=(800, 600), # Figure size in pixels (width, height)\n",
    "        fontsize: int=12, # Font size for annotations\n",
    "        xlim: tuple=(None, None), # Tuple of (min, max) values for x-axis limits\n",
    "        ylim: tuple=(None, None), # Tuple of (min, max) values for y-axis limits\n",
    "        ):\n",
    "    \"\"\"Plot MAE vs MSE scores with degree and window annotations for model comparison using Plotly\"\"\"\n",
    "    import plotly.graph_objects as go\n",
    "    import plotly.express as px\n",
    "    \n",
    "    # Get unique time steps - expected format 't+N' where N is 0-10\n",
    "    time_steps = []\n",
    "    for col in df.index.get_level_values(0).unique():\n",
    "        if isinstance(col, str) and col.startswith('t+'):\n",
    "            time_steps.append(col)\n",
    "    time_steps = sorted(time_steps, key=lambda x: int(x.split('+')[1]))\n",
    "    # Create a colormap\n",
    "    # Generate more colors by interpolating the Viridis colormap\n",
    "    n_colors = len(time_steps)\n",
    "    colors = px.colors.sample_colorscale('Viridis', n_colors)\n",
    "    # Create figure\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    # Plot each time step with different color\n",
    "    for t, color in zip(time_steps, colors):\n",
    "        mask = df.index.get_level_values(0) == t\n",
    "        df_t = df[mask]\n",
    "        \n",
    "        # Add scatter plot\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=df_t['mae'],\n",
    "            y=df_t['rmse'],\n",
    "            mode='markers+text',\n",
    "            name=t,\n",
    "            marker=dict(color=color, size=10),\n",
    "            text=[f'd={deg},w={win}' for (_, deg, win) in df_t.index],\n",
    "            textposition=\"top center\",\n",
    "            textfont=dict(size=fontsize),\n",
    "            hovertemplate=\"MAE: %{x:.3f}<br>RMSE: %{y:.3f}<br>%{text}<extra></extra>\"\n",
    "        ))\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        width=figsize[0],\n",
    "        height=figsize[1],\n",
    "        title='MAE vs RMSE for different degrees, windows and prediction horizons',\n",
    "        xaxis_title='MAE',\n",
    "        yaxis_title='RMSE',\n",
    "        legend_title='Prediction horizon',\n",
    "        hovermode='closest',\n",
    "        showlegend=True\n",
    "    )\n",
    "    \n",
    "    # Set axis limits if provided\n",
    "    if xlim[0] is not None or xlim[1] is not None:\n",
    "        fig.update_xaxes(range=xlim)\n",
    "    if ylim[0] is not None or ylim[1] is not None:\n",
    "        fig.update_yaxes(range=ylim)\n",
    "    \n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| eval: false\n",
    "plot_interactive_benchmark_scores(best_scores,)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Based on the scatter plot comparing MAE vs MSE metrics, we can conclude that polynomial regression with degree 2 and window sizes between 30-50 days provides the optimal predictions. This is evident from the cluster of points in the lower left corner of the plot, which indicates lower error rates for both metrics. Specifically, the combinations of degree=2 with windows around 40 days achieve the best balance between Mean Absolute Error and Mean Squared Error, suggesting these parameters offer the most accurate and stable predictions without overfitting the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save prediction data\n",
    "\n",
    "We save the best model for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| eval: false\n",
    "benchmark_ds = results_ds.sel(degree=2, window=slice(30,50))\n",
    "benchmark_ds.to_netcdf(DATA/'regression_benchmark.nc')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time series verification\n",
    "\n",
    "Now that we have identified the optimal model parameters, let's verify its performance by comparing the predicted discharge values with both observed values and MGB model predictions. This comparison will be done across the full 10-day prediction horizon to assess how well our model maintains its predictive power over time. We'll visualize these comparisons using time series plots that show the observed discharge, our model's predictions, and the MGB model predictions side by side.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def plot_prediction_comparison(\n",
    "        observed: xr.DataArray, # Time series of observed water discharge values\n",
    "        predicted: xr.Dataset, # Dataset containing predicted discharge values for each forecast horizon (t+1 to t+10)\n",
    "        mgb: xr.DataArray # Time series of MGB model water discharge predictions\n",
    "        ) -> plt.Figure:\n",
    "    \"\"\"Plot comparison between observed, predicted and MGB discharge values for a 10-day horizon.\"\"\"\n",
    "    fig, axes = plt.subplots(5, 2, figsize=(14, 10), sharex=True, sharey=True)\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i in range(0, 10):\n",
    "        y_obs = observed.to_series()\n",
    "        y_pred = predicted[f\"t+{i+1}\"].to_series()\n",
    "        y_mgb = mgb.to_series()\n",
    "        \n",
    "        rmse = np.sqrt(((y_obs - y_pred)**2).mean())\n",
    "        correlation = np.corrcoef(y_obs, y_pred)[0, 1]\n",
    "\n",
    "        axes[i].plot(y_obs, label=\"Q_obs\", color='blue')\n",
    "        axes[i].plot(y_pred, label=\"Q_pred\", color='red', linestyle='solid')\n",
    "        axes[i].plot(y_mgb, label=\"Q_mgb\", color='black')\n",
    "\n",
    "        axes[i].set_title(f\"Jour {i+1} RMSE={rmse:.2f}, Corr={correlation:.2f}\")\n",
    "        \n",
    "        axes[i].grid()\n",
    "        axes[i].legend()\n",
    "\n",
    "    plt.suptitle(\"Comparaison des valeurs réelles et prédites pour chaque jour de l'horizon de 10 jours\", fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We first need to reescale the MGB data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| eval: false\n",
    "rescaled_features = pd.DataFrame(features_scaler.inverse_transform(valid[x_col]), index=valid.index, columns=x_col)\n",
    "debit_mgb = rescaled_features[\"débit_mgb\"].to_xarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| eval: false\n",
    "fig = plot_prediction_comparison(\n",
    "    observed=results_ds[\"débit_insitu\"],\n",
    "    predicted=results_ds.sel(degree=1, window=50), \n",
    "    mgb=debit_mgb)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
