{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Benchmark Models\n",
    "\n",
    "This notebook implements simple benchmark models to compare against LSTM performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp benchmark_model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *\n",
    "from fastcore.basics import patch\n",
    "from pathlib import Path\n",
    "import hvplot.pandas\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.preprocessing import RobustScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.preprocessing import PolynomialFeatures"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will first open the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_PATH = Path(\"../testing_data\")\n",
    "\n",
    "df = pd.read_csv(\n",
    "    DATA_PATH/'hydro_example.csv', \n",
    "    usecols=['time', 'smoothed_rain', 'Q_mgb', 'Q_obs'], \n",
    "    index_col='time',\n",
    "    converters={\"time\": pd.to_datetime}\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now take a quick look to the data to see the trends and the expected improvement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def normalize(df):\n",
    "    return (df - df.min()) / (df.max() - df.min())\n",
    "\n",
    "normalize(df).plot()#.hvplot.line()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select feature and target columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_col, y_col = ['smoothed_rain','Q_mgb'], ['Q_obs']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_mask = df.index < '2019-01-01'\n",
    "train = df[train_mask]\n",
    "valid = df[~train_mask]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_scaler = RobustScaler()\n",
    "train[x_col] = features_scaler.fit_transform(train[x_col])\n",
    "valid[x_col] = features_scaler.transform(valid[x_col])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature generator\n",
    "> Feature generator that uses a sliding window of n previous timesteps and polynomial features to predict the next value, enabling learning of temporal patterns and non-linear relationships"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class FeatureAndTargetGenerator:\n",
    "    \"\"\"\n",
    "    Transforms time series data into feature matrices suitable for machine learning models.\n",
    "    Creates lagged features using a sliding window and optionally generates polynomial features\n",
    "    to capture non-linear relationships between variables.\n",
    "    \"\"\"\n",
    "    def __init__(self, context_len: int = 10, target_len: int = 10, poly_degree: int = 1):\n",
    "        self.context_len = context_len\n",
    "        self.target_len = target_len\n",
    "        self.poly_features = PolynomialFeatures(degree=poly_degree)\n",
    "        \n",
    "    def generate(self, df: pd.DataFrame, x_col: list[str], y_col: list[str]) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        X, y = df[x_col], df[y_col]\n",
    "        if 1 < self.poly_features.degree:\n",
    "            X = pd.DataFrame(self.poly_features.fit_transform(X), index=X.index)\n",
    "        X, y = self.generate_sliding_window_features(X, y)\n",
    "        return X, y\n",
    "\n",
    "\n",
    "    def generate_sliding_window_features(self, X: pd.DataFrame, y: pd.DataFrame) -> tuple[pd.DataFrame, pd.DataFrame]:\n",
    "        \"\"\"\n",
    "        Creates a feature matrix by combining multiple input variables and their lagged values.\n",
    "        For each time step t, takes values from t-window to t for each input variable\n",
    "        and combines them into a single feature vector. The target value is taken at time t.\n",
    "        This allows the model to learn patterns across multiple timesteps.\n",
    "        \"\"\"\n",
    "        features = []\n",
    "        targets = []\n",
    "        \n",
    "        for i in range(len(X) - self.context_len - self.target_len):\n",
    "            row_features = X.iloc[i:i + self.context_len]\n",
    "            features.append(row_features.values.reshape(-1))\n",
    "            row_targets = y.iloc[i + self.context_len: i + self.context_len + self.target_len]\n",
    "            targets.append(row_targets.values.reshape(-1))\n",
    "\n",
    "        features = pd.DataFrame(\n",
    "            index=X.index[self.context_len - 1:len(X) - self.target_len - 1],\n",
    "              data=features)\n",
    "        targets = pd.DataFrame(\n",
    "            index=y.index[self.context_len - 1:len(X) - self.target_len - 1],\n",
    "            data=targets,\n",
    "            columns=[f\"t+{i+1}\" for i in range(0, self.target_len)])\n",
    "        return features, targets\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example we can generate train and validation sets as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "generator = FeatureAndTargetGenerator(context_len=1, target_len=2, poly_degree=2)\n",
    "train_x, train_y = generator.generate(train, x_col, y_col)\n",
    "valid_x, valid_y = generator.generate(valid, x_col, y_col)\n",
    "train_x.head(3), train_y.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Benchmark model\n",
    "We chose a linear regression model as our benchmark because:\n",
    "1. When combined with our feature generator, it provides a simple yet effective baseline\n",
    "2. The polynomial features allow it to capture non-linear relationships in the data\n",
    "3. The sliding window enables learning of temporal patterns across multiple timesteps\n",
    "4. Linear regression models are highly interpretable and computationally efficient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class SimpleRegressionModel:\n",
    "    def __init__(self):\n",
    "        self.model = LinearRegression()\n",
    "    \n",
    "    def fit(self, X, y=None):\n",
    "        self.model.fit(X, y)\n",
    "        return self\n",
    "    \n",
    "    def predict(self, X):\n",
    "        return self.model.predict(X)\n",
    "    \n",
    "    def predict_as_dataframe(self, X, degree, context_len):\n",
    "        pred = self.predict(X)\n",
    "        pred = pd.DataFrame(pred, index=X.index, columns=[f\"t+{i+1}\" for i in range(0, pred.shape[1])])\n",
    "        pred.columns.name = \"forecast_horizon\"\n",
    "        pred.index.name = \"run_time\"\n",
    "        pred['degree'] = degree\n",
    "        pred['context_len'] = context_len\n",
    "        pred = pred.set_index(['degree', 'context_len'], append=True)\n",
    "        pred = pred.reorder_levels([\"degree\", \"context_len\", \"run_time\"]).sort_index()\n",
    "        return pred"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define and train a simple model as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleRegressionModel()\n",
    "model.fit(train_x, train_y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And we can get the raw prediction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.predict(valid_x)[:3]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Or the prediction with indexes as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pred = model.predict_as_dataframe(valid_x, degree=2, context_len=1)\n",
    "pred.head(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time index handling\n",
    "\n",
    "This section implements utilities for handling time indexes in forecasting data. We provide functions to: Convert between two data formats:\n",
    "- Forecast horizons as columns (e.g., t+1, t+2 columns)\n",
    "- Forecast horizons and times as row indices\n",
    "The first format is convenient for saving data, while the second is better suited for scoring and plotting since it explicitly tracks the actual forecast times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class ForecastTimeHandler:\n",
    "    \"\"\"\n",
    "    A utility class for handling forecast time transformations.\n",
    "\n",
    "    This class provides functionality to manipulate forecast time between different formats,\n",
    "    specifically handling the conversion between columnar forecast horizons and stacked time series formats.\n",
    "    It manages forecast horizons (e.g., 't+1', 't+2') and their corresponding timestamps.\n",
    "    \"\"\"\n",
    "    def __init__(\n",
    "            self,\n",
    "            stack_col_name: str = \"pred\" # Name of the column when columns are stacked\n",
    "            ):\n",
    "        self.stack_col_name = stack_col_name\n",
    "\n",
    "    def stack(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Stack the forecast horizon as index and add forecast time as index\"\"\"\n",
    "        df = df.copy()\n",
    "        if df.columns.name is None:\n",
    "            df.columns.name = \"forecast_horizon\"\n",
    "        df = self.transpose_forecast_horizon_as_index(df)\n",
    "        df = self.add_forecast_time_as_index(df)\n",
    "        return df\n",
    "    \n",
    "    def transpose_forecast_horizon_as_index(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        df=df.stack()\n",
    "        df = df.to_frame(self.stack_col_name)\n",
    "        return df\n",
    "\n",
    "    def add_forecast_time_as_index(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        def get_daily_timedeltas(forecast_horizons):\n",
    "            \"\"\"Extract timedelta day values from forecast horizons starting with 't+'\"\"\"\n",
    "            return [pd.Timedelta(days=int(fh.replace(\"t+\", \"\"))) for fh in forecast_horizons if fh.startswith('t+')]\n",
    "        df = df.copy()\n",
    "        forecast_horizon = df.index.get_level_values(\"forecast_horizon\")\n",
    "        timedeltas = get_daily_timedeltas(forecast_horizon)\n",
    "        df[\"forecast_time\"] = df.index.get_level_values(\"run_time\") + pd.Index(timedeltas)\n",
    "        df.set_index(\"forecast_time\", inplace=True, append=True)\n",
    "        return df\n",
    "    \n",
    "    def unstack(self, df: pd.DataFrame) -> pd.DataFrame:\n",
    "        \"\"\"Convert stacked forecast horizon index back to horizon-as-columns format\"\"\"\n",
    "        return df.reset_index(\"forecast_time\", drop=True)[self.stack_col_name].unstack(\"forecast_horizon\")\n",
    "    \n",
    "    def align(self, pred, obs):\n",
    "        \"\"\"Align the predictions and observations by forecast time\"\"\"\n",
    "        obs, pred = obs.copy(), pred.copy()\n",
    "        obs.index.name = \"forecast_time\"\n",
    "        pred, obs = pred.align(obs, join=\"outer\", axis=0)\n",
    "        return pred, obs\n",
    "    \n",
    "    def join(self, pred, obs):\n",
    "        \"\"\"Join the predictions and observations by forecast time\"\"\"\n",
    "        obs, pred = obs.copy(), pred.copy()\n",
    "        obs.index.name = \"forecast_time\"\n",
    "        return pred.join(obs, on=\"forecast_time\")\n",
    "\n",
    "    def align_as_xarray(self, pred, obs):\n",
    "        \"\"\"Align the predictions and observations by forecast horizon and return as xarray\"\"\"\n",
    "        if 1 != len(obs.columns):\n",
    "            raise ValueError(\"Observations must have only one column\")\n",
    "        obs_col = obs.columns[0]\n",
    "        pred_col = self.stack_col_name\n",
    "\n",
    "        obs, pred = obs.copy(), pred.copy()\n",
    "        pred = self.stack(pred)\n",
    "        obs.index.name = \"forecast_time\"\n",
    "        pred, obs = self.align(pred, obs)\n",
    "\n",
    "        self.stack_col_name = obs_col\n",
    "        obs = self.unstack(obs)\n",
    "        obs = obs.to_xarray().to_array(\"forecast_horizon\", name=obs_col)\n",
    "        self.stack_col_name = pred_col\n",
    "        pred = self.unstack(pred)\n",
    "        pred = pred.to_xarray().to_array(\"forecast_horizon\", name=pred_col)\n",
    "        \n",
    "        return pred, obs\n",
    "    \n",
    "    def align_and_join_as_xarray(self, pred, obs):\n",
    "        \"\"\"Align the predictions and observations by forecast horizon and join them as xarray\"\"\"\n",
    "        pred, obs = self.align_as_xarray(pred, obs)\n",
    "        return xr.merge([pred, obs])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The previous class can be used to add forecast time as index and to revert this operation. First we create an instance of the class giving the name of the column when forecast horizon will be set as index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_time_handler = ForecastTimeHandler(stack_col_name=\"pred\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can use the instance to stack the forecast horizon as index:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "stacked_pred = forecast_time_handler.stack(pred)\n",
    "stacked_pred.head(5)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can rever the operation as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "forecast_time_handler.align_and_join_as_xarray(pred, observations)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scoring\n",
    "\n",
    "Since we have generated multiple predictions with different parameters, we will select only the best performing models according to each metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class BenchmarkScores:\n",
    "    \"\"\"\n",
    "    A class to compute and compare different error metrics between predictions and observations.\n",
    "    \n",
    "    This class provides a flexible way to compute multiple error metrics between predicted and observed values.\n",
    "    New metrics can be easily added by implementing them as methods.\n",
    "    The new metrics can then be used by passing their method names as strings to compute_scores().\n",
    "    \n",
    "    Example:\n",
    "\n",
    "    ```python\n",
    "    class CustomBenchmarkScores(BenchmarkScores):\n",
    "        def mse(self, pred, obs):\n",
    "            error = ((pred - obs)**2).mean(dim=\"time\") \n",
    "            return error\n",
    "        \n",
    "    # Can then be used as:\n",
    "    scores = compute_scores(pred, obs, metrics=[\"mae\", \"rmse\", \"mse\"])\n",
    "    ```\n",
    "    \"\"\"\n",
    "    def __init__(self): pass\n",
    "\n",
    "\n",
    "    def compute_scores(self, predictions: pd.DataFrame, observations: pd.DataFrame, metrics: list[str]) -> pd.DataFrame:\n",
    "        \"\"\"Compute the scores for the predictions and observations.\"\"\"\n",
    "        forecast_time_handler = ForecastTimeHandler()\n",
    "        pred, obs = forecast_time_handler.align_as_xarray(predictions, observations)\n",
    "        \n",
    "        scores = []\n",
    "        for metric in metrics:\n",
    "            score = getattr(self, metric)(pred, obs)\n",
    "            scores.append(score.to_dataset(name=metric))\n",
    "        return xr.merge(scores).to_dataframe()\n",
    "    \n",
    "    def mae(self, pred, obs):\n",
    "        error = abs(pred - obs).mean(dim=\"run_time\")\n",
    "        return error\n",
    "\n",
    "    def rmse(self, pred, obs):\n",
    "        error = np.sqrt(((pred - obs)**2).mean(dim=\"run_time\"))\n",
    "        return error\n",
    "    \n",
    "    def nse(self, pred, obs):\n",
    "        \"\"\"\n",
    "        Calculate Nash-Sutcliffe Efficiency score.\n",
    "        \n",
    "        $NSE = 1 - \\\\frac{\\\\sum(pred - obs)^2}{\\\\sum(obs - \\\\overline{obs})^2}$\n",
    "        \n",
    "        NSE ranges from -inf to 1, with 1 being a perfect match\n",
    "        \"\"\"\n",
    "        numerator = ((pred - obs)**2).sum(dim=\"run_time\")\n",
    "        denominator = ((obs - obs.mean(dim=\"run_time\"))**2).sum(dim=\"run_time\")\n",
    "        error = 1 - (numerator / denominator)\n",
    "        return error\n",
    "    \n",
    "    def kge(self, pred, obs):\n",
    "        \"\"\"Calculate Kling-Gupta Efficiency score.\n",
    "        \n",
    "        $KGE = 1 - sqrt((r-1)^2 + (\\alpha-1)^2 + (\\beta-1)^2)$\n",
    "        \n",
    "        where:\n",
    "        - r = Pearson correlation coefficient\n",
    "        - $\\\\alpha = \\\\frac{std(pred)}{std(obs)}$ \n",
    "        - $\\\\beta = \\\\frac{mean(pred)}{mean(obs)}$\n",
    "        \n",
    "        KGE ranges from -inf to 1, with 1 being a perfect match\n",
    "        \"\"\"\n",
    "        # Calculate components\n",
    "        r = xr.corr(pred, obs, dim=\"run_time\")\n",
    "        alpha = pred.std(dim=\"run_time\") / obs.std(dim=\"run_time\")\n",
    "        beta = pred.mean(dim=\"run_time\") / obs.mean(dim=\"run_time\")\n",
    "        \n",
    "        # Calculate KGE\n",
    "        error = 1 - np.sqrt((r - 1)**2 + (alpha - 1)**2 + (beta - 1)**2)\n",
    "        return error\n",
    "\n",
    "    def find_nbest_scores(\n",
    "            self,\n",
    "            df: pd.DataFrame, # Dataframe containing scores as columns\n",
    "            how: dict[str, str], # Dictionary of how to find the best scores for each variable\n",
    "            n: int=2 # Number of best scores to find\n",
    "        ):\n",
    "        \"\"\"Given a dataset containing scores as variables,find the n best scores for each score.\"\"\"\n",
    "        assert all(how in [\"min\", \"max\"] for how in how.values()), \"how must be either 'min' or 'max'\"\n",
    "        df = df.reorder_levels([\"forecast_horizon\"] + [level for level in df.index.names if level != \"forecast_horizon\"])\n",
    "        best_scores_coords = []\n",
    "        for score, how in how.items():\n",
    "            ascending = how == \"min\"\n",
    "            best_scores_forecast_horizon = self._find_best_scores_by_forecast_horizon(df[score], n, ascending)\n",
    "            best_scores_coords.extend(best_scores_forecast_horizon.values.tolist())\n",
    "        best_scores_coords = pd.MultiIndex.from_tuples(best_scores_coords, names=df.index.names)\n",
    "        n_best_scores = df.loc[best_scores_coords]\n",
    "        n_best_scores = n_best_scores.sort_index().drop_duplicates()\n",
    "        return n_best_scores\n",
    "\n",
    "    def _find_best_scores_by_forecast_horizon(self, serie: pd.Series, n, ascending: bool):\n",
    "        \"\"\"For each unique forecast horizon in a multi-indexed Series, find indices of the n best values based on ascending/descending sort.\"\"\"\n",
    "        sorted = serie.sort_values(ascending=ascending)\n",
    "        nbest_values = []\n",
    "        for v in sorted.index.get_level_values(\"forecast_horizon\").unique():\n",
    "            nbest_values.append(sorted.loc[[v]].head(n))\n",
    "        nbest_values = pd.concat(nbest_values)\n",
    "        return nbest_values.index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_scores = BenchmarkScores()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = benchmark_scores.compute_scores(predictions, observations, [\"mae\", \"rmse\", \"nse\", \"kge\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_scores.find_nbest_scores(_, how={\"mae\": \"min\", \"rmse\": \"min\", \"nse\": \"max\", \"kge\": \"max\"}, n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores_ds = benchmark_scores.compute_scores(\n",
    "    predictions,\n",
    "    observations[\"Q_obs\"],\n",
    "    [\"mae\", \"rmse\", \"nse\", \"kge\"])\n",
    "best_scores = benchmark_scores.find_nbest_scores(\n",
    "    scores_ds,\n",
    "    how={\"mae\": \"min\", \"rmse\": \"min\", \"nse\": \"max\", \"kge\": \"max\"},\n",
    "    n=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model training and comparison\n",
    "\n",
    "We can now make a simple prediction to compare different window sizes and differen polynomial degrees"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "predictions = []\n",
    "scores = []\n",
    "for degree in range(1, 2+1):\n",
    "    for window in range(10, 20+1, 10):\n",
    "        feature_generator = FeatureGenerator(context_window=window, target_window=4, degree=degree)        \n",
    "        train_x, train_y = feature_generator.generate(train, x_col, y_col)\n",
    "        valid_x, valid_y = feature_generator.generate(valid, x_col, y_col)\n",
    "\n",
    "        model = SimpleRegressionModel()\n",
    "        model.fit(train_x, train_y)\n",
    "        \n",
    "        pred = model.predict_as_dataframe(valid_x, degree, window)\n",
    "        predictions.append(pred)\n",
    "\n",
    "predictions = pd.concat(predictions)#.to_xarray()\n",
    "observations = valid[y_col]\n",
    "# predictions = pd.concat(predictions).to_xarray()\n",
    "# observations = valid[y_col].to_xarray().sel(time=slice(predictions.run_time.min(), predictions.run_time.max()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Static plotting\n",
    "\n",
    "Finally we will plot the error so we can visualize the best results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def plot_static_benchmark_scores(\n",
    "        df: pd.DataFrame, # Dataframe with polynomial degree and window as index and mae and mse as columns\n",
    "        figsize: tuple=(8, 7), # Figure size in inches (width, height)\n",
    "        fontsize: int=7, # Font size for annotations\n",
    "        xlim: tuple=(None, None), # Tuple of (min, max) values for x-axis limits\n",
    "        ylim: tuple=(None, None), # Tuple of (min, max) values for y-axis limits\n",
    "        ):\n",
    "    \"\"\"Plot MAE vs MSE scores with degree and window annotations for model comparison\"\"\"\n",
    "    fig, ax = plt.subplots(figsize=figsize)\n",
    "    \n",
    "    # Get unique time steps - expected format 't+N' where N is 0-10\n",
    "    time_steps = []\n",
    "    for col in df.index.get_level_values(0).unique():\n",
    "        if isinstance(col, str) and col.startswith('t+'):\n",
    "            time_steps.append(col)\n",
    "    time_steps = sorted(time_steps, key=lambda x: int(x.split('+')[1]))\n",
    "    \n",
    "    # Create a colormap\n",
    "    colors = plt.cm.viridis(np.linspace(0, 1, len(time_steps)))\n",
    "    \n",
    "    # Plot each time step with different color\n",
    "    for t, color in zip(time_steps, colors):\n",
    "        mask = df.index.get_level_values(0) == t\n",
    "        df_t = df[mask]\n",
    "        scatter = ax.scatter(df_t['mae'], df_t['rmse'], label=t, color=color)\n",
    "        \n",
    "        # Add annotations for each point\n",
    "        for (_, deg, win), (mae, rmse) in zip(df_t.index, df_t[['mae', 'rmse']].values):\n",
    "            ax.annotate(f'd={deg},w={win}', (mae, rmse), \n",
    "                       xytext=(5, 5), textcoords='offset points', \n",
    "                       fontsize=fontsize, color=color)\n",
    "\n",
    "    ax.set(xlabel='MAE', ylabel='RMSE', \n",
    "           title='MAE vs RMSE for different degrees, windows and prediction horizons')\n",
    "    ax.legend(title='Prediction horizon', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.xlim(*xlim)\n",
    "    plt.ylim(*ylim)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| eval: false\n",
    "plot_static_benchmark_scores(best_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Interactive plotting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def plot_interactive_benchmark_scores(\n",
    "        df: pd.DataFrame, # Dataframe with polynomial degree and window as index and mae and mse as columns\n",
    "        figsize: tuple=(800, 600), # Figure size in pixels (width, height)\n",
    "        fontsize: int=12, # Font size for annotations\n",
    "        xlim: tuple=(None, None), # Tuple of (min, max) values for x-axis limits\n",
    "        ylim: tuple=(None, None), # Tuple of (min, max) values for y-axis limits\n",
    "        ):\n",
    "    \"\"\"Plot MAE vs MSE scores with degree and window annotations for model comparison using Plotly\"\"\"\n",
    "    import plotly.graph_objects as go\n",
    "    import plotly.express as px\n",
    "    \n",
    "    # Get unique time steps - expected format 't+N' where N is 0-10\n",
    "    time_steps = []\n",
    "    for col in df.index.get_level_values(0).unique():\n",
    "        if isinstance(col, str) and col.startswith('t+'):\n",
    "            time_steps.append(col)\n",
    "    time_steps = sorted(time_steps, key=lambda x: int(x.split('+')[1]))\n",
    "    # Create a colormap\n",
    "    # Generate more colors by interpolating the Viridis colormap\n",
    "    n_colors = len(time_steps)\n",
    "    colors = px.colors.sample_colorscale('Viridis', n_colors)\n",
    "    # Create figure\n",
    "    fig = go.Figure()\n",
    "    \n",
    "    # Plot each time step with different color\n",
    "    for t, color in zip(time_steps, colors):\n",
    "        mask = df.index.get_level_values(0) == t\n",
    "        df_t = df[mask]\n",
    "        \n",
    "        # Add scatter plot\n",
    "        fig.add_trace(go.Scatter(\n",
    "            x=df_t['mae'],\n",
    "            y=df_t['rmse'],\n",
    "            mode='markers+text',\n",
    "            name=t,\n",
    "            marker=dict(color=color, size=10),\n",
    "            text=[f'd={deg},w={win}' for (_, deg, win) in df_t.index],\n",
    "            textposition=\"top center\",\n",
    "            textfont=dict(size=fontsize),\n",
    "            hovertemplate=\"MAE: %{x:.3f}<br>RMSE: %{y:.3f}<br>%{text}<extra></extra>\"\n",
    "        ))\n",
    "    \n",
    "    # Update layout\n",
    "    fig.update_layout(\n",
    "        width=figsize[0],\n",
    "        height=figsize[1],\n",
    "        title='MAE vs RMSE for different degrees, windows and prediction horizons',\n",
    "        xaxis_title='MAE',\n",
    "        yaxis_title='RMSE',\n",
    "        legend_title='Prediction horizon',\n",
    "        hovermode='closest',\n",
    "        showlegend=True\n",
    "    )\n",
    "    \n",
    "    # Set axis limits if provided\n",
    "    if xlim[0] is not None or xlim[1] is not None:\n",
    "        fig.update_xaxes(range=xlim)\n",
    "    if ylim[0] is not None or ylim[1] is not None:\n",
    "        fig.update_yaxes(range=ylim)\n",
    "    \n",
    "    return fig\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_interactive_benchmark_scores(best_scores,)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time series verification\n",
    "\n",
    "Now that we have identified the optimal model parameters, let's verify its performance by comparing the predicted discharge values with both observed values and MGB model predictions. This comparison will be done across the full prediction horizon to assess how well our model maintains its predictive power over time. We'll visualize these comparisons using time series plots that show the observed discharge, our model's predictions, and the MGB model predictions side by side.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def plot_prediction_comparison(\n",
    "        observed: xr.DataArray, # Time series of observed water discharge values\n",
    "        predicted: xr.Dataset, # Dataset containing predicted discharge values for each forecast horizon (t+i)\n",
    "        mgb: xr.DataArray, # Time series of MGB model water discharge predictions\n",
    "        best_model: dict=None,\n",
    "        scores: xr.Dataset=None # Dictionary containg the t+i as key and the best model fit degree and window as value\n",
    "        ) -> plt.Figure:\n",
    "    \"\"\"Plot comparison between observed, predicted and MGB discharge values for a i-day horizon.\"\"\"\n",
    "    n_horizon = len(predicted.data_vars)\n",
    "    fig, axes = plt.subplots(int(n_horizon/2), 2, figsize=(20, int(n_horizon/2*5)), sharex=True, sharey=True)\n",
    "    axes = axes.flatten()\n",
    "\n",
    "    for i in range(0, n_horizon):\n",
    "        y_obs = observed.to_series()\n",
    "        y_pred = predicted[f\"t+{i+1}\"]\n",
    "        if best_model is not None:\n",
    "            y_pred = y_pred.sel(**best_model[f\"t+{i+1}\"])\n",
    "        y_pred = y_pred.to_series()\n",
    "        y_mgb = mgb.to_series()\n",
    "\n",
    "\n",
    "        if scores is not None:\n",
    "            score = scores.sel(forecast_horizon=f\"t+{i+1}\")\n",
    "            if best_model is not None:\n",
    "                score = score.sel(**best_model[f\"t+{i+1}\"])\n",
    "            score = score.to_array().to_series().to_dict()\n",
    "\n",
    "        axes[i].plot(y_obs, label=\"Q_obs\", color='blue')\n",
    "        axes[i].plot(y_pred, label=\"Q_pred\", color='red', linestyle='solid')\n",
    "        axes[i].plot(y_mgb, label=\"Q_mgb\", color='black')\n",
    "\n",
    "        score_str = \"\".join(f\" {k.upper()}={v:.2f}\" for k, v in score.items())\n",
    "        if best_model is not None:\n",
    "            model_str = \"\".join(f\" {k.capitalize()}={v}\" for k, v in best_model[f\"t+{i+1}\"].items())\n",
    "        else:\n",
    "            model_str = \"\"\n",
    "        axes[i].set_title(f\"Jour {i+1} {model_str}\\n {score_str}\")\n",
    "        \n",
    "        axes[i].grid()\n",
    "        axes[i].legend()\n",
    "\n",
    "    plt.suptitle(\"Comparaison des valeurs réelles et prédites pour chaque jour de l'horizon de 10 jours\", fontsize=14)\n",
    "    plt.tight_layout()\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the previous analysis we can decides which model works the best for each predicted time step. We will use the best model for each time step to plot the prediction comparison. As example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_models={\n",
    "    \"t+1\": {\"degree\": 2, \"ctx_window\": 20}, \n",
    "    \"t+2\": {\"degree\": 2, \"ctx_window\": 20}, \n",
    "    \"t+3\": {\"degree\": 2, \"ctx_window\": 10},\n",
    "    \"t+4\": {\"degree\": 2, \"ctx_window\": 10},\n",
    "    }\n",
    "fig = plot_prediction_comparison(\n",
    "    observed=observations[\"Q_obs\"], \n",
    "    predicted=predictions, \n",
    "    best_model=best_models, \n",
    "    mgb=df[~train_mask][\"Q_mgb\"].to_xarray(),\n",
    "    scores=scores_ds\n",
    "    )\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save prediction data\n",
    "\n",
    "We save the best model for later use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "#| eval: false\n",
    "results = predictions.merge(observations)\n",
    "benchmark_ds = results.sel(degree=2, ctx_window=slice(30,50))\n",
    "# benchmark_ds.to_netcdf(DATA_PATH/'regression_benchmark.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
