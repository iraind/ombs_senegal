{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# Fundamental functions for time series modeling using deep learning methods in pytorch\n",
    "\n",
    "The objective of this notebook is to provaide with the basic building blocks to be able to easily test different Deep Learning approaches on tabular time series using pytorch. The notebook includes a basic on how to use the functions to train a NN model.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp time_series_deepl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "\n",
    "\n",
    "DATA_PATH = Path(\"../testing_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Data Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "We will first open the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data =pd.read_csv(DATA_PATH / \"hydro_example.csv\", parse_dates=True, index_col=\"time\")\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "Now we will split data into coherent groups"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def split_by_date(\n",
    "        data: pd.DataFrame, # Input dataframe containing time series data\n",
    "        val_dates: tuple,   # Tuple of (start_date, end_date) for validation set\n",
    "        test_dates: tuple   # Tuple of (start_date, end_date) for test set\n",
    "        ) -> tuple[pd.DataFrame, pd.DataFrame, pd.DataFrame]:\n",
    "    \"\"\"Split time series data into train, validation and test sets based on date ranges.\"\"\"\n",
    "    val_data = data[val_dates[0]:val_dates[1]]\n",
    "    test_data = data[test_dates[0]:test_dates[1]]\n",
    "    train_data = data[~(data.index.isin(val_data.index) | data.index.isin(test_data.index))]\n",
    "    print(f\"Approximate data repartition:\\n\"\n",
    "          f\"Train: {train_data.shape[0]/data.shape[0]:.2%}\\n\"\n",
    "          f\"Validation: {val_data.shape[0]/data.shape[0]:.2%}\\n\" \n",
    "          f\"Test: {test_data.shape[0]/data.shape[0]:.2%}\")\n",
    "    return train_data, val_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid, test = split_by_date(data, val_dates=(\"2012-01-01\", \"2012-12-31\"), test_dates=(\"2013-01-01\", \"2014-12-31\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "Now lets define the feature and the target columns and divide data in feature and targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cols = [\"smoothed_rain\",\"Q_mgb\"]\n",
    "y_cols = [\"Q_obs\"]\n",
    "\n",
    "x_train, y_train = train[x_cols], train[y_cols]\n",
    "x_valid, y_valid = valid[x_cols], valid[y_cols]\n",
    "x_test, y_test = test[x_cols], test[y_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "Now we will fit the scaler based only on train data. This ensures that:\n",
    "1. No information from the validation/test data sets leaks to into the scaling process\n",
    "2. All data is scaled consistently using the same parameters\n",
    "3. The model sees new data scaled in the same way as it was trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from sklearn.preprocessing import RobustScaler\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_scaler, target_scaler = RobustScaler(), RobustScaler()\n",
    "_, _ = feature_scaler.fit_transform(x_train), target_scaler.fit_transform(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15",
   "metadata": {},
   "source": [
    "Finally, we'll create a custom dataset class to handle our time series data. This class will create sequences of input features (simulation discharge and rainfall) and target values (observed discharge)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class HydroDataset(Dataset):\n",
    "    def __init__(\n",
    "            self,\n",
    "            x: pd.DataFrame,\n",
    "            y: pd.DataFrame, \n",
    "            ctx_len: int, \n",
    "            pred_len: int = 10, \n",
    "            x_transform: callable = None,\n",
    "            y_transform: callable = None):\n",
    "        \n",
    "        if x_transform is None:\n",
    "            self.features = x.copy()\n",
    "        else:\n",
    "            self.features = pd.DataFrame(x_transform(x), columns=x.columns, index=x.index)\n",
    "        if y_transform is None:\n",
    "            self.targets = y.copy()\n",
    "        else:\n",
    "            self.targets = pd.DataFrame(y_transform(y), columns=y.columns, index=y.index)\n",
    "        \n",
    "        self.context_length = ctx_len\n",
    "        self.prediction_length = pred_len\n",
    "        self.x_transform = x_transform\n",
    "        self.y_transform = y_transform\n",
    "        \n",
    "    def __len__(self):\n",
    "        return self.features.shape[0] - self.context_length - self.prediction_length + 1\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        # Get sequence of features\n",
    "        features = self.features[idx:idx + self.context_length]\n",
    "        # Get target (next value after sequence)\n",
    "        targets = self.targets[idx + self.context_length:idx + self.context_length + self.prediction_length]\n",
    "        return torch.FloatTensor(features.values), torch.FloatTensor(targets.values)\n",
    "    \n",
    "    def get_t0(self, idx):\n",
    "        \"\"\"Get the t+0 in the sequence from where forecast is made.\"\"\"\n",
    "        return self.features.index[idx + self.context_length-1]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "We can easily instantiate the dataset as follows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = HydroDataset(\n",
    "    x=x_train,\n",
    "    y=y_train,\n",
    "    ctx_len=1,\n",
    "    pred_len=1,\n",
    "    x_transform=feature_scaler.transform,\n",
    "    y_transform=target_scaler.transform\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "The total training samples are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "len(train_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "Is it possible to easly get a training sample as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset[5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "And also to the the t+0 for any item"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset.get_t0(1000)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25",
   "metadata": {},
   "source": [
    "## Model example\n",
    "\n",
    "For the sake of example, we will define the simplest NN we possibly can in PyTorch, which is a simple linear model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleNN(nn.Module):\n",
    "    def __init__(self, input_dim, output_dim):\n",
    "        super(SimpleNN, self).__init__()\n",
    "        self.linear = nn.Linear(input_dim, output_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        batch_size = x.shape[0]\n",
    "        x = x.reshape(batch_size, -1)\n",
    "        out = self.linear(x)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27",
   "metadata": {},
   "source": [
    "## Model training\n",
    "\n",
    "Now we will define a basic learner class to handle the training process. This class will be used to train the model and evaluate its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "class Learner:\n",
    "    def __init__(self,\n",
    "                 model: nn.Module, # model to train\n",
    "                 train_loader: DataLoader, # data loader for training data\n",
    "                 val_loader: DataLoader, # data loader for validation data\n",
    "                 criterion: nn.Module = nn.MSELoss(), # loss function to optimize\n",
    "                 optimizer: torch.optim.Optimizer = torch.optim.Adam, # optimizer class to use for training\n",
    "                 log_dir: str = None, # directory to save tensorboard logs,\n",
    "                 verbose: bool = True # whether to print training progress\n",
    "                 ) -> None:\n",
    "        self.model = model\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.writer = None if log_dir is None else SummaryWriter(log_dir)\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def fit(self, lr=0.001, epochs=10):\n",
    "        optimizer = self.optimizer(self.model.parameters(), lr=lr)\n",
    "        for epoch in tqdm(range(epochs), desc='Training epochs'):\n",
    "            # Training\n",
    "            self.model.train()\n",
    "            epoch_loss = 0\n",
    "            for batch_X, batch_y in self.train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.model(batch_X)\n",
    "                loss = self.criterion(outputs, batch_y.squeeze())\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "            avg_train_loss = epoch_loss/len(self.train_loader)\n",
    "            if self.writer is not None:\n",
    "                self.writer.add_scalar('Training Loss/epoch', avg_train_loss, epoch)\n",
    "\n",
    "            # Validation\n",
    "            self.model.eval()\n",
    "            val_loss = 0\n",
    "            with torch.no_grad():\n",
    "                for batch_X, batch_y in self.val_loader:\n",
    "                    val_outputs = self.model(batch_X)\n",
    "                    val_loss += self.criterion(val_outputs, batch_y.squeeze()).item()\n",
    "            \n",
    "            avg_val_loss = val_loss/len(self.val_loader)\n",
    "            if self.writer is not None:\n",
    "                self.writer.add_scalar('Validation Loss/epoch', avg_val_loss, epoch)\n",
    "\n",
    "            if self.verbose:\n",
    "                print(f'Epoch {epoch+1}, Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')\n",
    "    \n",
    "    def predict_values(self, dl: DataLoader):\n",
    "        self.model.eval()\n",
    "        predictions = []\n",
    "        targets = []\n",
    "        with torch.no_grad():\n",
    "            for batch_X, batch_y in dl:\n",
    "                batch_pred = self.model(batch_X).cpu().numpy()\n",
    "                predictions.append(batch_pred)\n",
    "\n",
    "        predictions = np.vstack(predictions)\n",
    "        return predictions\n",
    "    \n",
    "    def predict(self, dl: DataLoader, inverse_transform: callable=None) -> pd.DataFrame:\n",
    "        \"\"\"Make predictions and return them as a pandas DataFrame with proper indexing and column names.\"\"\"\n",
    "\n",
    "        predictions = self.predict_values(dl)\n",
    "        \n",
    "        # Get the indices from the dataset\n",
    "        indices = []\n",
    "        for batch_X, _ in dl:\n",
    "            # Assuming the dataset has get_t0 method to get the forecast start time\n",
    "            if hasattr(dl.dataset, 'get_t0'):\n",
    "                for i in range(len(batch_X)):\n",
    "                    idx = len(indices)\n",
    "                    indices.append(dl.dataset.get_t0(idx))\n",
    "        \n",
    "        if inverse_transform is None:\n",
    "            predictions = predictions.reshape(-1, predictions.shape[-1])\n",
    "        else:\n",
    "            predictions = inverse_transform(predictions.reshape(-1, predictions.shape[-1]))\n",
    "        \n",
    "        n_horizons = predictions.shape[1] if len(predictions.shape) > 1 else 1\n",
    "        column_names = [f\"t+{i+1}\" for i in range(0, n_horizons)]\n",
    "        \n",
    "        predictions_df = pd.DataFrame(predictions, index=indices, columns=column_names)\n",
    "        \n",
    "        return predictions_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "## Model training example\n",
    "\n",
    "Lets see a simple example of how we can train a neural network."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30",
   "metadata": {},
   "source": [
    "First we will create our Datasets and Dataloarders based on the data we splitted above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "\n",
    "context_len=3\n",
    "prediction_len=2\n",
    "x_transform=feature_scaler.transform\n",
    "y_transform=target_scaler.transform\n",
    "\n",
    "train_dataset = HydroDataset(x=x_train, y=y_train, ctx_len=context_len, pred_len=prediction_len, x_transform=x_transform, y_transform=y_transform)\n",
    "valid_dataset = HydroDataset(x=x_valid, y=y_valid, ctx_len=context_len, pred_len=prediction_len, x_transform=x_transform, y_transform=y_transform)\n",
    "test_dataset = HydroDataset(x=x_test, y=y_test, ctx_len=context_len, pred_len=prediction_len, x_transform=x_transform, y_transform=y_transform)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "valid_loader = DataLoader(valid_dataset, batch_size=batch_size)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32",
   "metadata": {},
   "source": [
    "We can now instantiate the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = SimpleNN(input_dim=len(x_cols)*context_len, output_dim=prediction_len)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34",
   "metadata": {},
   "source": [
    "And finally we can instantiate the learner and fit our data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "learner = Learner(model=model, train_loader=train_loader, val_loader=valid_loader)\n",
    "learner.fit(lr=0.001, epochs=3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36",
   "metadata": {},
   "source": [
    "Lets now see the prediction. There are two possible ways. Predicting only the values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = learner.predict_values(test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38",
   "metadata": {},
   "source": [
    "Getting the prediction with the timestamp and column name. This allow us also to scale back to the original values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = learner.predict(test_loader, inverse_transform=target_scaler.inverse_transform)\n",
    "y_pred.head(4)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40",
   "metadata": {},
   "source": [
    "We will now add the observation and the mgb simulation so we can plot the result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred[\"obs\"] = y_test.loc[y_pred.index]\n",
    "y_pred[\"mgb\"] = x_test[\"Q_mgb\"].loc[y_pred.index]\n",
    "y_pred.plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
