[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "ombs_senegal",
    "section": "",
    "text": "This file will become your README and also the index of your documentation.",
    "crumbs": [
      "ombs_senegal"
    ]
  },
  {
    "objectID": "index.html#developer-guide",
    "href": "index.html#developer-guide",
    "title": "ombs_senegal",
    "section": "Developer Guide",
    "text": "Developer Guide\nIf you are new to using nbdev here are some useful pointers to get you started.\n\nInstall ombs_senegal in Development mode\n# make sure ombs_senegal package is installed in development mode\n$ pip install -e .\n\n# make changes under nbs/ directory\n# ...\n\n# compile to have changes apply to ombs_senegal\n$ nbdev_prepare",
    "crumbs": [
      "ombs_senegal"
    ]
  },
  {
    "objectID": "index.html#usage",
    "href": "index.html#usage",
    "title": "ombs_senegal",
    "section": "Usage",
    "text": "Usage\n\nInstallation\nInstall latest from the GitHub repository:\n$ pip install git+https://github.com/iraind/ombs_senegal.git\nor from conda\n$ conda install -c iraind ombs_senegal\nor from pypi\n$ pip install ombs_senegal\n\n\nDocumentation\nDocumentation can be found hosted on this GitHub repository’s pages. Additionally you can find package manager specific guidelines on conda and pypi respectively.",
    "crumbs": [
      "ombs_senegal"
    ]
  },
  {
    "objectID": "index.html#how-to-use",
    "href": "index.html#how-to-use",
    "title": "ombs_senegal",
    "section": "How to use",
    "text": "How to use\nFill me in please! Don’t forget code examples:\n\n1+1\n\n2",
    "crumbs": [
      "ombs_senegal"
    ]
  },
  {
    "objectID": "deepl.core.html",
    "href": "deepl.core.html",
    "title": "Fundamental functions for time series modeling using deep learning methods in pytorch",
    "section": "",
    "text": "from pathlib import Path\n\n\nDATA_PATH = Path(\"../testing_data\")",
    "crumbs": [
      "Deep Learning",
      "Fundamental functions for time series modeling using deep learning methods in pytorch"
    ]
  },
  {
    "objectID": "deepl.core.html#data-preprocessing",
    "href": "deepl.core.html#data-preprocessing",
    "title": "Fundamental functions for time series modeling using deep learning methods in pytorch",
    "section": "Data Preprocessing",
    "text": "Data Preprocessing\nWe will first open the data\n\ndata =pd.read_csv(DATA_PATH / \"hydro_example.csv\", parse_dates=True, index_col=\"time\")\ndata.head(5)\n\nNow we will split data into coherent groups\n\nsource\n\nsplit_by_date\n\n split_by_date (data:pandas.core.frame.DataFrame, val_dates:tuple,\n                test_dates:tuple)\n\nSplit time series data into train, validation and test sets based on date ranges.\n\n\n\n\n\n\n\n\n\nType\nDetails\n\n\n\n\ndata\nDataFrame\nInput dataframe containing time series data\n\n\nval_dates\ntuple\nTuple of (start_date, end_date) for validation set\n\n\ntest_dates\ntuple\nTuple of (start_date, end_date) for test set\n\n\nReturns\ntuple\n\n\n\n\n\ntrain, valid, test = split_by_date(data, val_dates=(\"2012-01-01\", \"2012-12-31\"), test_dates=(\"2013-01-01\", \"2014-12-31\"))\n\nNow lets define the feature and the target columns and divide data in feature and targets\n\nx_cols = [\"smoothed_rain\",\"Q_mgb\"]\ny_cols = [\"Q_obs\"]\n\nx_train, y_train = train[x_cols], train[y_cols]\nx_valid, y_valid = valid[x_cols], valid[y_cols]\nx_test, y_test = test[x_cols], test[y_cols]\n\nNow we will fit the scaler based only on train data. This ensures that: 1. No information from the validation/test data sets leaks to into the scaling process 2. All data is scaled consistently using the same parameters 3. The model sees new data scaled in the same way as it was trained\n\nfeature_scaler, target_scaler = RobustScaler(), RobustScaler()\n_, _ = feature_scaler.fit_transform(x_train), target_scaler.fit_transform(y_train)\n\nFinally, we’ll create a custom dataset class to handle our time series data. This class will create sequences of input features (simulation discharge and rainfall) and target values (observed discharge).\n\nsource\n\n\nHydroDataset\n\n HydroDataset (x:pandas.core.frame.DataFrame,\n               y:pandas.core.frame.DataFrame, ctx_len:int,\n               pred_len:int=10, x_transform:&lt;built-\n               infunctioncallable&gt;=None, y_transform:&lt;built-\n               infunctioncallable&gt;=None)\n\n*An abstract class representing a :class:Dataset.\nAll datasets that represent a map from keys to data samples should subclass it. All subclasses should overwrite :meth:__getitem__, supporting fetching a data sample for a given key. Subclasses could also optionally overwrite :meth:__len__, which is expected to return the size of the dataset by many :class:~torch.utils.data.Sampler implementations and the default options of :class:~torch.utils.data.DataLoader. Subclasses could also optionally implement :meth:__getitems__, for speedup batched samples loading. This method accepts list of indices of samples of batch and returns list of samples.\n.. note:: :class:~torch.utils.data.DataLoader by default constructs an index sampler that yields integral indices. To make it work with a map-style dataset with non-integral indices/keys, a custom sampler must be provided.*\nWe can easily instantiate the dataset as follows\n\ntrain_dataset = HydroDataset(\n    x=x_train,\n    y=y_train,\n    ctx_len=1,\n    pred_len=1,\n    x_transform=feature_scaler.transform,\n    y_transform=target_scaler.transform\n    )\n\nThe total training samples are\n\nlen(train_dataset)\n\nIs it possible to easly get a training sample as follows:\n\ntrain_dataset[5]\n\nAnd also to the the t+0 for any item\n\ntrain_dataset.get_t0(1000)",
    "crumbs": [
      "Deep Learning",
      "Fundamental functions for time series modeling using deep learning methods in pytorch"
    ]
  },
  {
    "objectID": "deepl.core.html#model-example",
    "href": "deepl.core.html#model-example",
    "title": "Fundamental functions for time series modeling using deep learning methods in pytorch",
    "section": "Model example",
    "text": "Model example\nFor the sake of example, we will define the simplest NN we possibly can in PyTorch, which is a simple linear model.\n\nclass SimpleNN(nn.Module):\n    def __init__(self, input_dim, output_dim):\n        super(SimpleNN, self).__init__()\n        self.linear = nn.Linear(input_dim, output_dim)\n\n    def forward(self, x):\n        batch_size = x.shape[0]\n        x = x.reshape(batch_size, -1)\n        out = self.linear(x)\n        return out",
    "crumbs": [
      "Deep Learning",
      "Fundamental functions for time series modeling using deep learning methods in pytorch"
    ]
  },
  {
    "objectID": "deepl.core.html#model-training",
    "href": "deepl.core.html#model-training",
    "title": "Fundamental functions for time series modeling using deep learning methods in pytorch",
    "section": "Model training",
    "text": "Model training\nNow we will define a basic learner class to handle the training process. This class will be used to train the model and evaluate its performance.\n\nsource\n\nLearner\n\n Learner (model:torch.nn.modules.module.Module,\n          train_loader:torch.utils.data.dataloader.DataLoader,\n          val_loader:torch.utils.data.dataloader.DataLoader,\n          criterion:torch.nn.modules.module.Module=MSELoss(),\n          optimizer:torch.optim.optimizer.Optimizer=&lt;class\n          'torch.optim.adam.Adam'&gt;, log_dir:str=None, verbose:bool=True)\n\nInitialize self. See help(type(self)) for accurate signature.\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\nmodel\nModule\n\nmodel to train\n\n\ntrain_loader\nDataLoader\n\ndata loader for training data\n\n\nval_loader\nDataLoader\n\ndata loader for validation data\n\n\ncriterion\nModule\nMSELoss()\nloss function to optimize\n\n\noptimizer\nOptimizer\nAdam\noptimizer class to use for training\n\n\nlog_dir\nstr\nNone\ndirectory to save tensorboard logs,\n\n\nverbose\nbool\nTrue\nwhether to print training progress\n\n\nReturns\nNone",
    "crumbs": [
      "Deep Learning",
      "Fundamental functions for time series modeling using deep learning methods in pytorch"
    ]
  },
  {
    "objectID": "deepl.core.html#model-training-example",
    "href": "deepl.core.html#model-training-example",
    "title": "Fundamental functions for time series modeling using deep learning methods in pytorch",
    "section": "Model training example",
    "text": "Model training example\nLets see a simple example of how we can train a neural network.\nFirst we will create our Datasets and Dataloarders based on the data we splitted above\n\nbatch_size = 32\n\ncontext_len=3\nprediction_len=2\nx_transform=feature_scaler.transform\ny_transform=target_scaler.transform\n\ntrain_dataset = HydroDataset(x=x_train, y=y_train, ctx_len=context_len, pred_len=prediction_len, x_transform=x_transform, y_transform=y_transform)\nvalid_dataset = HydroDataset(x=x_valid, y=y_valid, ctx_len=context_len, pred_len=prediction_len, x_transform=x_transform, y_transform=y_transform)\ntest_dataset = HydroDataset(x=x_test, y=y_test, ctx_len=context_len, pred_len=prediction_len, x_transform=x_transform, y_transform=y_transform)\n\ntrain_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\nvalid_loader = DataLoader(valid_dataset, batch_size=batch_size)\ntest_loader = DataLoader(test_dataset, batch_size=batch_size)\n\nWe can now instantiate the model\n\nmodel = SimpleNN(input_dim=len(x_cols)*context_len, output_dim=prediction_len)\n\nAnd finally we can instantiate the learner and fit our data\n\nlearner = Learner(model=model, train_loader=train_loader, val_loader=valid_loader)\nlearner.fit(lr=0.001, epochs=3)\n\nLets now see the prediction. There are two possible ways. Predicting only the values.\n\ny_pred = learner.predict_values(test_loader)\n\nGetting the prediction with the timestamp and column name. This allow us also to scale back to the original values.\n\ny_pred = learner.predict(test_loader, inverse_transform=target_scaler.inverse_transform)\ny_pred.head(4)\n\nWe will now add the observation and the mgb simulation so we can plot the result.\n\ny_pred[\"obs\"] = y_test.loc[y_pred.index]\ny_pred[\"mgb\"] = x_test[\"Q_mgb\"].loc[y_pred.index]\ny_pred.plot()",
    "crumbs": [
      "Deep Learning",
      "Fundamental functions for time series modeling using deep learning methods in pytorch"
    ]
  },
  {
    "objectID": "dataprep.region.html",
    "href": "dataprep.region.html",
    "title": "Region",
    "section": "",
    "text": "source",
    "crumbs": [
      "Data Preparation",
      "Region"
    ]
  },
  {
    "objectID": "dataprep.region.html#example",
    "href": "dataprep.region.html#example",
    "title": "Region",
    "section": "Example",
    "text": "Example\n\nInput\n\n\n\n\n\n\n\n\n\n\n\nOutput",
    "crumbs": [
      "Data Preparation",
      "Region"
    ]
  },
  {
    "objectID": "dataprep.season.html",
    "href": "dataprep.season.html",
    "title": "Seasonal Handling for Hydrological Flow Prediction",
    "section": "",
    "text": "Data loading\n\ndata = pd.read_csv(\n    DATA_PATH/'hydro_example.csv', \n    usecols=['time', OBS_COL], \n    index_col='time',\n    converters={\"time\": pd.to_datetime}\n    )\ndata = data['2012-01-01':]\n\n\n\nSeassonal model\nWe create a class that is able to: - Compute seasonal patterns based on week of year - Remove seasonality from data - Add seasonality back to data\n\nsource\n\n\nSeasonalityHandler\n\n SeasonalityHandler ()\n\n*Class to handle seasonality operations in time series data.\nThis class provides methods to: - Compute seasonal patterns based on week of year - Remove seasonality from data - Add seasonality back to data\nAttributes: seasonal_pattern: pd.DataFrame The computed seasonal pattern, indexed by week of year*\nFirst we create an instance:\n\nseasonality_handler = SeasonalityHandler()\n\n\nsource\n\n\nSeasonalityHandler.compute_seasonal_pattern\n\n SeasonalityHandler.compute_seasonal_pattern\n                                              (data:pandas.core.frame.Data\n                                              Frame)\n\nCompute mean values for each week of the year to capture seasonal patterns.\nNow we compute the seasonality of the data as follows:\n\nseason = seasonality_handler.compute_seasonal_pattern(data)\n\n\nsource\n\n\nSeasonalityHandler.remove_seasonality\n\n SeasonalityHandler.remove_seasonality (data:pandas.core.frame.DataFrame)\n\nRemove seasonality from the data.\nIts also possible to remove the seasonality to our data as follows\n\ndeseasonalized_data = seasonality_handler.remove_seasonality(data)\ndeseasonalized_data.plot()\n\n\n\n\n\n\n\n\n\nsource\n\n\nSeasonalityHandler.add_seasonality\n\n SeasonalityHandler.add_seasonality (data:pandas.core.frame.DataFrame)\n\nAdd seasonality back to the data.\nWe can also add the seasonality back\n\nseasonality_handler.add_seasonality(deseasonalized_data).head(3)\n\n\n\n\n\n\n\n\nQ_obs\n\n\ntime\n\n\n\n\n\n2012-01-01\n68.839996\n\n\n2012-01-02\n67.500000\n\n\n2012-01-03\n67.349998\n\n\n\n\n\n\n\n\nsource\n\n\nSeasonalityHandler.append_season\n\n SeasonalityHandler.append_season (data:pandas.core.frame.DataFrame)\n\nAppend the seasonality to the data.\nOr we can append the seasonal data as a new column as follows\n\nseasonality_handler.append_season(data).head(3)\n\n\n\n\n\n\n\n\nQ_obs\nseason\n\n\ntime\n\n\n\n\n\n\n2012-01-01\n68.839996\n90.025156\n\n\n2012-01-02\n67.500000\n79.543198\n\n\n2012-01-03\n67.349998\n79.543198",
    "crumbs": [
      "Data Preparation",
      "Seasonal Handling for Hydrological Flow Prediction"
    ]
  },
  {
    "objectID": "dataprep.features.html",
    "href": "dataprep.features.html",
    "title": "Feature and Targets",
    "section": "",
    "text": "source\n\nFeatureAndTargetGenerator\n\n FeatureAndTargetGenerator (context_len:int=10, target_len:int=10,\n                            poly_degree:int=1)\n\nTransforms time series data into feature matrices suitable for machine learning models. Creates lagged features using a sliding window and optionally generates polynomial features to capture non-linear relationships between variables. It also creates a target vector for the number of timesteps to predict.\n\n\n\n\n\n\n\n\n\n\nType\nDefault\nDetails\n\n\n\n\ncontext_len\nint\n10\nnumber of previous timesteps to use as features\n\n\ntarget_len\nint\n10\nnumber of timesteps to predict\n\n\npoly_degree\nint\n1\ndegree of polynomial features\n\n\n\nWe will first open the data\n\nDATA_PATH = Path(\"../testing_data\")\n\ndata = pd.read_csv(\n    DATA_PATH/'hydro_example.csv', \n    usecols=['time', 'smoothed_rain', 'Q_mgb', 'Q_obs'], \n    index_col='time',\n    converters={\"time\": pd.to_datetime}\n    )\n\nThen we need to create an instance of the generator setting the context and the target length and the polynomial degree.\n\ngenerator = FeatureAndTargetGenerator(context_len=1, target_len=2, poly_degree=2)\n\nThen we can generate the feature and target matrixes as follows\n\nsource\n\n\nFeatureAndTargetGenerator.generate\n\n FeatureAndTargetGenerator.generate (df:pandas.core.frame.DataFrame,\n                                     x_col:list[str], y_col:list[str])\n\nGenerates a feature matrix and target vector from the input data.\n\nx_col, y_col = ['smoothed_rain','Q_mgb'], ['Q_obs']\ndata_x, data_y = generator.generate(data, x_col=x_col, y_col=y_col)\n\nThe generated data will look as follows\n\ndata_x.head(3)\n\n\n\n\n\n\n\n\n0\n1\n2\n3\n4\n5\n\n\ntime\n\n\n\n\n\n\n\n\n\n\n2012-01-01\n1.0\n0.299868\n15.22\n0.089921\n4.563995\n231.6484\n\n\n2012-01-02\n1.0\n0.299767\n14.84\n0.089860\n4.448548\n220.2256\n\n\n2012-01-03\n1.0\n0.265321\n14.48\n0.070395\n3.841846\n209.6704\n\n\n\n\n\n\n\n\ndata_y.head(3)\n\n\n\n\n\n\n\n\nt+1\nt+2\n\n\ntime\n\n\n\n\n\n\n2012-01-01\n67.500000\n67.349998\n\n\n2012-01-02\n67.349998\n66.800003\n\n\n2012-01-03\n66.800003\n66.739998",
    "crumbs": [
      "Data Preparation",
      "Feature and Targets"
    ]
  }
]