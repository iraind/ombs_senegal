{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# L'objectif est d'expérimenter différentes tailles de fenêtres temporelles (window_size) pour trouver celle qui donne les meilleures performances.\n",
    "\n",
    "## Méthodologie\n",
    "* Définir une liste de tailles de fenêtres (window_size) à tester, par exemple [30, 60, 90, 120].\n",
    "* Créer des séquences avec chaque window_size et un prediction_size fixe.\\\\\n",
    "* Entraîner le modèle LSTM sur chaque fenêtre.\n",
    "* Évaluer les performances avec RMSE, MAE et R².\n",
    "* Comparer les performances pour choisir la meilleure fenêtre.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Preguntas\n",
    "\n",
    "- ¿Por qué, si la validation loss oscila tanto, no paras el entrenamiento antes?\n",
    "- ¿Que es RobustNormalization?\n",
    "- Añadir tensor board para seguir el entrenamiento\n",
    "- Quizas no séa relevante para el entrenamiento y la predicción del modelo pero ¿el hecho de que robust scaler haga que haya lluvia negativa no va a afectar? Quizas habría que revisarlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install -q torch --index-url https://download.pytorch.org/whl/cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import pandas as pd\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from ombs_senegal.time_series_deepl import Learner, HydroDataset, split_by_date\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "\n",
    "DATA_PATH = Path(\"../../data\")\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(\n",
    "    DATA_PATH/'data_cumul.csv', \n",
    "    sep=';', \n",
    "    usecols=['time', 'débit_insitu', 'P_cumul_7j', 'débit_mgb'], \n",
    "    index_col='time',\n",
    "    converters={\"time\": pd.to_datetime}\n",
    "    )\n",
    "data = data[\"2012-01-01\":]\n",
    "data[\"mois\"] = data.index.month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid, test = split_by_date(data, val_dates=(\"2018-01-01\", \"2018-12-31\"), test_dates=(\"2019-01-01\", \"2020-12-31\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "Now lets define the feature and the target columns and divide data in feature and targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cols = [\"débit_mgb\",\"P_cumul_7j\", \"mois\"]\n",
    "y_cols = [\"débit_insitu\"]\n",
    "\n",
    "x_train, y_train = train[x_cols], train[y_cols]\n",
    "x_valid, y_valid = valid[x_cols], valid[y_cols]\n",
    "x_test, y_test = test[x_cols], test[y_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "Now we will fit the scaler based only on train data. This ensures that:\n",
    "1. No information from the validation/test data sets leaks to into the scaling process\n",
    "2. All data is scaled consistently using the same parameters\n",
    "3. The model sees new data scaled in the same way as it was trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_scaler, target_scaler = RobustScaler(), RobustScaler()\n",
    "_, _ = feature_scaler.fit_transform(x_train), target_scaler.fit_transform(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11",
   "metadata": {},
   "source": [
    "## Model definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "#### Multi layer perceptron (MLP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRegularizedMLP(nn.Module):\n",
    "    def __init__(self, input_size, prediction_size):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 64)\n",
    "        self.norm = nn.LayerNorm(64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc2 = nn.Linear(64, prediction_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "#### Long Short Term Memory (LSTM)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRegularizedLSTM(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, prediction_size):\n",
    "        super().__init__()\n",
    "        self.lstm = nn.LSTM(input_size, hidden_size, batch_first=True)\n",
    "        self.norm = nn.LayerNorm(hidden_size)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc = nn.Linear(hidden_size, prediction_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, (hn, _) = self.lstm(x)\n",
    "        x = hn[-1]\n",
    "        x = self.norm(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, prediction_size):\n",
    "        super().__init__()\n",
    "        self.bilstm1 = nn.LSTM(input_size, 256, bidirectional=True, batch_first=True)\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        self.bilstm2 = nn.LSTM(512, 128, bidirectional=True, batch_first=True)\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "        self.lstm = nn.LSTM(256, 64, batch_first=True)\n",
    "        self.dropout3 = nn.Dropout(0.2)\n",
    "        self.dense1 = nn.Linear(64, 128)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout4 = nn.Dropout(0.2)\n",
    "        self.dense2 = nn.Linear(128, prediction_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.bilstm1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x, _ = self.bilstm2(x)\n",
    "        x = self.dropout2(x)\n",
    "        #x, _ = self.lstm(x)\n",
    "        x, (hn, _) = self.lstm(x)\n",
    "        x = hn[-1]   \n",
    "        x = self.dropout3(x)\n",
    "        x = self.dense1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout4(x)\n",
    "        x = self.dense2(x)\n",
    "        return x\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "#### Gated Recurrent Units (GRU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRegularizedGRU(nn.Module):\n",
    "    def __init__(self, input_size, hidden_size, prediction_size):\n",
    "        super().__init__()\n",
    "        self.gru = nn.GRU(input_size, hidden_size, batch_first=True)\n",
    "        self.norm = nn.LayerNorm(hidden_size)\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc = nn.Linear(hidden_size, prediction_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        _, hn = self.gru(x)\n",
    "        x = hn[-1]\n",
    "        x = self.norm(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19",
   "metadata": {},
   "source": [
    "#### Convolutional Neural Networks (CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class TemporalCausalCNN(nn.Module):\n",
    "    def __init__(self, input_channels, window_size, prediction_size):\n",
    "        super().__init__()\n",
    "        self.window_size = window_size\n",
    "\n",
    "        self.conv1 = nn.Conv1d(input_channels, 32, kernel_size=3)\n",
    "        self.norm1 = nn.LayerNorm([window_size, 32])\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "\n",
    "        self.conv2 = nn.Conv1d(32, 64, kernel_size=3)\n",
    "        self.norm2 = nn.LayerNorm([window_size, 64])\n",
    "\n",
    "        self.global_pool = nn.AdaptiveAvgPool1d(1)\n",
    "        self.fc = nn.Linear(64, prediction_size)\n",
    "        self.relu = nn.ReLU()\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Entrada: (batch, time, variables) → permutar para Conv1d\n",
    "        x = x.permute(0, 2, 1)  # (B, C, T)\n",
    "\n",
    "        # Padding causal antes de cada convolución\n",
    "        x = F.pad(x, (2, 0))  # padding izquierda = kernel_size - 1\n",
    "        x = self.conv1(x)     # (B, 32, T)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.norm1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "\n",
    "        x = F.pad(x, (2, 0))\n",
    "        x = self.conv2(x)     # (B, 64, T)\n",
    "        x = x.permute(0, 2, 1)\n",
    "        x = self.norm2(x)\n",
    "        x = self.relu(x)\n",
    "        x = x.permute(0, 2, 1)\n",
    "\n",
    "        x = self.global_pool(x).squeeze(-1)  # (B, 64)\n",
    "        x = self.fc(x)                       # (B, prediction_size)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21",
   "metadata": {},
   "source": [
    "## Trainning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "from ombs_senegal.benchmark_model import BenchmarkScores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_scores = BenchmarkScores()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 🔹 Listes des tailles de fenêtres à tester\n",
    "context_sizes = [15]#, 30, 60, 90]\n",
    "batch_size = 32\n",
    "learning_rate = 0.0003\n",
    "epochs=1\n",
    "\n",
    "prediction_size = 10  # Fixe (peut être ajusté)\n",
    "x_transform=feature_scaler.transform\n",
    "y_transform=target_scaler.transform\n",
    "results = []\n",
    "models = []\n",
    "\n",
    "# 🔹 Boucle sur différentes tailles de fenêtres\n",
    "for context_size in context_sizes:\n",
    "    print(f\"\\n🟢 Test avec window_size = {context_size}\")\n",
    "\n",
    "    train_dataset = HydroDataset(x=x_train, y=y_train, ctx_len=context_size, pred_len=prediction_size, x_transform=x_transform, y_transform=y_transform)\n",
    "    valid_dataset = HydroDataset(x=x_valid, y=y_valid, ctx_len=context_size, pred_len=prediction_size, x_transform=x_transform, y_transform=y_transform)\n",
    "    test_dataset = HydroDataset(x=x_test, y=y_test, ctx_len=context_size, pred_len=prediction_size, x_transform=x_transform, y_transform=y_transform)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=batch_size)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "    # 🔹 Vérification des dimensions\n",
    "    # model = LSTMModel(len(x_cols), prediction_size).to(DEVICE)\n",
    "    #model = SimpleRegularizedLSTM(len(x_cols), 64, prediction_size).to(DEVICE)\n",
    "    # model = SimpleRegularizedGRU(len(x_cols), 64, prediction_size).to(DEVICE)\n",
    "    # model = TemporalCausalCNN(len(x_cols), context_size, prediction_size).to(DEVICE)\n",
    "    model = SimpleRegularizedMLP(len(x_cols)*context_size, prediction_size).to(DEVICE)\n",
    "    learner = Learner(model=model, train_loader=train_loader, val_loader=valid_loader)\n",
    "    learner.fit(lr=learning_rate, epochs=epochs)\n",
    "\n",
    "    y_pred = learner.predict(test_loader, inverse_transform=target_scaler.inverse_transform)\n",
    "\n",
    "    y_pred.index.name = \"time\"\n",
    "    y_pred[\"model\"] = model.__class__.__name__\n",
    "    y_pred.set_index([\"model\"], append=True, inplace=True)\n",
    "    y_pred = y_pred.to_xarray()\n",
    "    scores = benchmark_scores.compute_scores(y_pred, y_test.to_xarray()[y_cols[0]], metrics=[\"mae\", \"rmse\", \"nse\", \"kge\"])\n",
    "\n",
    "    mean_scores = {s.upper(): round(float(scores[s].mean().values), 2) for s in scores.data_vars}\n",
    "    print(f\"📊 Résultats pour window_size={context_size} -> {mean_scores}\")\n",
    "\n",
    "    # 🔹 Stocker les résultats\n",
    "    results.append({\"ctx_size\": context_size, **mean_scores})\n",
    "\n",
    "# 🔹 Afficher le meilleur résultat\n",
    "best = min(results, key=lambda x: x[\"RMSE\"])  # Choix basé sur le RMSE le plus bas\n",
    "print(f\"\\n✅ Meilleure fenêtre : {best[\"ctx_size\"]} avec RMSE={best[\"RMSE\"]}, MAE={best[\"MAE\"]}, NSE={best[\"NSE\"]}, KGE={best[\"KGE\"]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29",
   "metadata": {},
   "source": [
    "- MLP: Meilleure fenêtre : 60 avec RMSE=156.864, MAE=78.964, R²=0.861\n",
    "- CNN: Meilleure fenêtre : 30 avec RMSE=191.998, MAE=92.670, R²=0.786\n",
    "- GRU: Meilleure fenêtre : 60 avec RMSE=212.033, MAE=108.959, R²=0.746\n",
    "- Simple LSTM: Meilleure fenêtre : 90 avec RMSE=197.514, MAE=104.173, R²=0.785"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 🔹 Fonction pour calculer le PBIAS\n",
    "def pbias(y_true, y_pred):\n",
    "    return 100 * np.sum(y_pred - y_true) / np.sum(y_true)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31",
   "metadata": {},
   "source": [
    "## Learning rate finder development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_find(self, start_lr=1e-7, end_lr=10, num_iter=100, step_mode=\"exp\", show_plot=True):\n",
    "        \"\"\"Find a good learning rate by training with exponentially growing lr\n",
    "            source: https://github.com/fastai/fastai1/blob/master/fastai/train.py#L33\n",
    "\n",
    "        \n",
    "        Args:\n",
    "            start_lr (float): Starting learning rate\n",
    "            end_lr (float): Maximum learning rate\n",
    "            num_iter (int): Number of iterations to run\n",
    "            step_mode (str): \"exp\" for exponential increase, \"linear\" for linear increase\n",
    "            show_plot (bool): Whether to display the loss plot\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (optimal_lr, learning_rates, losses)\n",
    "        \"\"\"\n",
    "        # Save the original model state\n",
    "        original_state = {\n",
    "            'model': self.model.state_dict(),\n",
    "            'optimizer': self.optimizer\n",
    "        }\n",
    "        \n",
    "        # Initialize optimizer with start_lr\n",
    "        optimizer = self.optimizer(self.model.parameters(), lr=start_lr)\n",
    "        \n",
    "        # Calculate the multiplication factor for each step\n",
    "        if step_mode == \"exp\":\n",
    "            gamma = (end_lr / start_lr) ** (1 / num_iter)\n",
    "        else:\n",
    "            gamma = (end_lr - start_lr) / num_iter\n",
    "            \n",
    "        scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma) if step_mode == \"exp\" else None\n",
    "        \n",
    "        learning_rates = []\n",
    "        losses = []\n",
    "        best_loss = float('inf')\n",
    "        \n",
    "        # Create iterator for training data\n",
    "        iterator = iter(self.train_loader)\n",
    "        \n",
    "        for iteration in range(num_iter):\n",
    "            try:\n",
    "                batch_X, batch_y = next(iterator)\n",
    "            except StopIteration:\n",
    "                iterator = iter(self.train_loader)\n",
    "                batch_X, batch_y = next(iterator)\n",
    "                \n",
    "            # Forward pass\n",
    "            self.model.train()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = self.model(batch_X)\n",
    "            loss = self.criterion(outputs, batch_y.squeeze())\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Store the values\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            learning_rates.append(current_lr)\n",
    "            losses.append(loss.item())\n",
    "            \n",
    "            # Update learning rate\n",
    "            if step_mode == \"exp\":\n",
    "                scheduler.step()\n",
    "            else:\n",
    "                for param_group in optimizer.param_groups:\n",
    "                    param_group['lr'] = start_lr + (gamma * (iteration + 1))\n",
    "            \n",
    "            # Stop if the loss is exploding\n",
    "            if iteration > 0 and losses[-1] > 4 * best_loss:\n",
    "                break\n",
    "                \n",
    "            if losses[-1] < best_loss:\n",
    "                best_loss = losses[-1]\n",
    "        \n",
    "        # Restore the original model state\n",
    "        self.model.load_state_dict(original_state['model'])\n",
    "        \n",
    "        if show_plot:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.plot(learning_rates, losses)\n",
    "            plt.xscale('log')\n",
    "            plt.xlabel('Learning Rate (log scale)')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.title('Learning Rate Finder')\n",
    "            plt.show()\n",
    "            \n",
    "        # Find the point of steepest descent\n",
    "        smoothed_losses = np.array(losses)\n",
    "        min_grad_idx = np.gradient(smoothed_losses).argmin()\n",
    "        optimal_lr = learning_rates[min_grad_idx]\n",
    "            \n",
    "        return optimal_lr, learning_rates, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
