{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "import hvplot.pandas\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ombs_senegal.region import get_region_mask\n",
    "\n",
    "\n",
    "DATA_PATH = Path(\"../../data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "from ombs_senegal.benchmark_model import FeatureGenerator, SimpleRegressionModel, BenchmarkScores\n",
    "from ombs_senegal.benchmark_model import plot_interactive_benchmark_scores, plot_prediction_comparison\n",
    "from ombs_senegal.season import SeasonalityHandler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    DATA_PATH/'data_cumul.csv', \n",
    "    sep=';', \n",
    "    usecols=['time', 'débit_insitu', 'débit_mgb'], \n",
    "    index_col='time',\n",
    "    converters={\"time\": pd.to_datetime}\n",
    "    )\n",
    "\n",
    "tamsat_daily_total = xr.load_dataset(DATA_PATH/\"tamsat_sub4_senegal_daily_total.nc\")\n",
    "\n",
    "data = pd.merge(df, tamsat_daily_total[\"rfe\"].to_dataframe(), left_index=True, right_index=True)\n",
    "data[\"débit_insitu_x\"] = data[\"débit_insitu\"].copy()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Preprocess data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Select feature and target columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_col, y_col = [\"débit_insitu_x\"], ['débit_insitu']\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train = data[:\"2018-12-31\"]\n",
    "test = data[\"2019-01-01\":]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Add target season column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# season_handler =SeasonalityHandler()\n",
    "# _ = season_handler.compute_seasonal_pattern(train[y_col])\n",
    "# train = season_handler.remove_seasonality(train)\n",
    "# train = season_handler.append_season(train)\n",
    "# test = season_handler.append_season(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Smooth data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth(df, window=7, missing_val=0): return df.rolling(window=window).sum().fillna(missing_val)\n",
    "\n",
    "data[\"rfe\"] = smooth(data[\"rfe\"], window=7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "features_scaler = RobustScaler()\n",
    "\n",
    "#train[x_col] = features_scaler.fit_transform(train[x_col])\n",
    "#test[x_col] = features_scaler.transform(test[x_col])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "predictions = []\n",
    "for degree in range(1, 10):\n",
    "    for window in range(1, 10):\n",
    "        feature_generator = FeatureGenerator(context_window=window, target_window=10, degree=degree)        \n",
    "        train_x, train_y = feature_generator.generate(train, x_col, y_col)\n",
    "        test_x, _ = feature_generator.generate(test, x_col, y_col)\n",
    "\n",
    "        model = SimpleRegressionModel()\n",
    "        model.fit(train_x, train_y)\n",
    "        predictions.append(model.predict_as_dataframe(test_x, degree=degree, ctx_window=window))\n",
    "\n",
    "\n",
    "predictions = pd.concat(predictions).reorder_levels(['degree', 'ctx_window', 'time']).to_xarray()\n",
    "observations = test[y_col[0]].to_xarray().sel(time=slice(predictions.time.min(), predictions.time.max()))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reseasonalize(ds, season_handler):\n",
    "    index = ds.to_dataframe().reset_index([\"degree\", \"ctx_window\"])[[\"degree\", \"ctx_window\"]]\n",
    "    index_cols = list(index.columns)\n",
    "    df = season_handler.add_seasonality(ds.to_dataframe().reset_index(index_cols, drop=True))\n",
    "    df[index_cols] = index\n",
    "    df = df.set_index(index_cols, append=True)\n",
    "    return df.to_xarray()\n",
    "\n",
    "# predictions = reseasonalize(predictions, season_handler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_scores = BenchmarkScores()\n",
    "scores_ds = benchmark_scores.compute_scores(\n",
    "    predictions,\n",
    "    observations,\n",
    "    [\"mae\", \"rmse\", \"nse\", \"kge\"])\n",
    "best_scores = benchmark_scores.find_nbest_scores(\n",
    "    scores_ds,\n",
    "    how={\"mae\": \"min\", \"rmse\": \"min\", \"nse\": \"max\", \"kge\": \"max\"},\n",
    "    n=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_interactive_benchmark_scores(best_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to be able to choose the best model we will analize three possibilities. \n",
    "- Average classic scores such as MAE and RMSE\n",
    "- Average Hydrological scores such as NSE and KGE\n",
    "- Average all the metrics scores. By this means we will normalize MAE and RMSE and inverse them being 1 the best and 0 the worst."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_average_score(scores, how=\"max\"):\n",
    "    \"\"\"Returns best model configuration based on averaged normalized scores across metrics.\"\"\"\n",
    "    metric_averaged_scores = scores.to_array().mean(dim=\"variable\")\n",
    "    \n",
    "    best_configuration = benchmark_scores.find_nbest_scores(\n",
    "        metric_averaged_scores.to_dataset(name=\"score\"), \n",
    "        how={\"score\": how}, \n",
    "        n=1\n",
    "    )\n",
    "    best_model_idx = {}\n",
    "    for idx, row in best_configuration.reset_index().iterrows():\n",
    "        best_model_idx[row[\"forecast_horizon\"]] = {\"degree\": row[\"degree\"], \"ctx_window\": row[\"ctx_window\"]}\n",
    "    return best_model_idx"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now get the best models for classic scores and hydrological scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_classic = get_best_average_score(scores_ds[[\"mae\", \"rmse\"]], how=\"min\")\n",
    "best_model_hydro = get_best_average_score(scores_ds[[\"nse\", \"kge\"]])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we will get the best models based on average score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def normalize_metrics(ds):\n",
    "    dims = [\"degree\", \"ctx_window\"]\n",
    "    return 1 - (ds - ds.min(dim=dims))/(ds.max(dim=dims) - ds.min(dim=dims))\n",
    "\n",
    "normalized_scores = scores_ds.copy()\n",
    "normalized_scores[[\"mae\", \"rmse\"]] = normalize_metrics(normalized_scores[[\"mae\", \"rmse\"]])\n",
    "best_model_avg = get_best_average_score(normalized_scores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can now plot the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_prediction_comparison(\n",
    "    observed=observations, \n",
    "    predicted=predictions, \n",
    "    best_model=best_model_avg,\n",
    "    mgb=test[\"débit_mgb\"].to_xarray(),\n",
    "    scores=scores_ds\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The analysis reveals distinct patterns in model performance across different metrics. While the model optimized for hydrological scores shows unique behavior, the model selected based on classic metrics closely aligns with the averaged score model's predictions. This alignment may be attributed to similarities in how these scores are calculated.\n",
    "\n",
    "Based on our visual analysis, we observe two key patterns:\n",
    "\n",
    "- With a 15-day smoothing window, the best performing model varies depending on the forecast horizon, though it generally corresponds to the model with the highest averaged score\n",
    "- With a 60-day smoothing window, the model with the best averaged score consistently outperforms other models across all forecast horizons\n",
    "\n",
    "The visual inspection further validates that the 60-day smoothing window, which shows the strongest correlation with hydrological metrics, produces the most accurate predictions overall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_model_avg"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_predictions = predictions.sel(degree=2, ctx_window=10).to_array(\"forecast_horizon\", name=\"pred\")\n",
    "benchmark_predictions = benchmark_predictions.expand_dims({\"model\": [\"Regression\"]})\n",
    "observations.name = \"obs\"\n",
    "benchmark_scores = scores_ds.sel(degree=2, ctx_window=10).to_array(\"score\", name=\"scores\")\n",
    "benchmark_scores = benchmark_scores.expand_dims({\"model\": [\"Regression\"]})\n",
    "benchmark_results = xr.merge([\n",
    "    benchmark_predictions,\n",
    "    observations,\n",
    "    benchmark_scores])\n",
    "benchmark_results.to_netcdf(DATA_PATH/'tamsat_regression_benchmark.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
