{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import xarray as xr\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from sklearn.preprocessing import RobustScaler\n",
    "\n",
    "from ombs_senegal.benchmark_model import BenchmarkScores\n",
    "from ombs_senegal.time_series_deepl import Learner, HydroDataset, split_by_date\n",
    "\n",
    "\n",
    "DATA_PATH = Path(\"../../data\")\n",
    "DEVICE = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\n",
    "    DATA_PATH/'data_cumul.csv', \n",
    "    sep=';', \n",
    "    usecols=['time', 'dÃ©bit_insitu', 'dÃ©bit_mgb', 'P_mean'], \n",
    "    index_col='time',\n",
    "    converters={\"time\": pd.to_datetime}\n",
    "    )\n",
    "\n",
    "tamsat_daily_total = xr.load_dataset(DATA_PATH/\"tamsat_sub_poly_daily_total.nc\")\n",
    "\n",
    "data = pd.merge(df, tamsat_daily_total[\"rfe\"].to_dataframe(), left_index=True, right_index=True)\n",
    "\n",
    "data = data[\"2012-01-01\":]\n",
    "data['month'] = data.index.month\n",
    "data['month_sin'] = np.sin(2 * np.pi * data.index.month / 12)\n",
    "data['month_cos'] = np.cos(2 * np.pi * data.index.month / 12)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def smooth(df, window=7, missing_val=0): return df.rolling(window=window).sum().fillna(missing_val)\n",
    "\n",
    "data[\"rfe_7d\"] = smooth(data[\"rfe\"])\n",
    "data[\"rfe_60d\"] = smooth(data[\"rfe\"], 60)\n",
    "data[\"imerg_7d\"] = smooth(data[\"P_mean\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, valid, test = split_by_date(data, val_dates=(\"2018-01-01\", \"2018-12-31\"), test_dates=(\"2019-01-01\", \"2020-12-31\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5",
   "metadata": {},
   "source": [
    "Now lets define the feature and the target columns and divide data in feature and targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_cols = [\"dÃ©bit_mgb\",\"rfe_7d\", \"month\"]\n",
    "y_cols = [\"dÃ©bit_insitu\"]\n",
    "\n",
    "x_train, y_train = train[x_cols], train[y_cols]\n",
    "x_valid, y_valid = valid[x_cols], valid[y_cols]\n",
    "x_test, y_test = test[x_cols], test[y_cols]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7",
   "metadata": {},
   "source": [
    "Now we will fit the scaler based only on train data. This ensures that:\n",
    "1. No information from the validation/test data sets leaks to into the scaling process\n",
    "2. All data is scaled consistently using the same parameters\n",
    "3. The model sees new data scaled in the same way as it was trained"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8",
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_scaler, target_scaler = RobustScaler(), RobustScaler()\n",
    "_, _ = feature_scaler.fit_transform(x_train), target_scaler.fit_transform(y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9",
   "metadata": {},
   "source": [
    "## Model definition"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "Based on the model benchmarking we have already done and on the fact that there is not many available data, we choose a simple Multi Layer Perceptron as model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleRegularizedMLP(nn.Module):\n",
    "    def __init__(self, input_size, prediction_size):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(input_size, 64)\n",
    "        self.norm = nn.LayerNorm(64)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.3)\n",
    "        self.fc2 = nn.Linear(64, prediction_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = self.fc1(x)\n",
    "        x = self.norm(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout(x)\n",
    "        x = self.fc2(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "## Trainning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ðŸ”¹ Listes des tailles de fenÃªtres Ã  tester\n",
    "context_sizes = [50]\n",
    "batch_size = 32\n",
    "learning_rate = 0.0003\n",
    "epochs=20\n",
    "\n",
    "prediction_size = 10  # Fixe (peut Ãªtre ajustÃ©)\n",
    "x_transform=feature_scaler.transform\n",
    "y_transform=target_scaler.transform\n",
    "results = []\n",
    "models = []\n",
    "\n",
    "benchmark_scores = BenchmarkScores()\n",
    "\n",
    "# ðŸ”¹ Boucle sur diffÃ©rentes tailles de fenÃªtres\n",
    "for context_size in context_sizes:\n",
    "    print(f\"\\nðŸŸ¢ Test avec window_size = {context_size}\")\n",
    "\n",
    "    train_dataset = HydroDataset(x=x_train, y=y_train, ctx_len=context_size, pred_len=prediction_size, x_transform=x_transform, y_transform=y_transform)\n",
    "    valid_dataset = HydroDataset(x=x_valid, y=y_valid, ctx_len=context_size, pred_len=prediction_size, x_transform=x_transform, y_transform=y_transform)\n",
    "    test_dataset = HydroDataset(x=x_test, y=y_test, ctx_len=context_size, pred_len=prediction_size, x_transform=x_transform, y_transform=y_transform)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "    valid_loader = DataLoader(valid_dataset, batch_size=batch_size)\n",
    "    test_loader = DataLoader(test_dataset, batch_size=batch_size)\n",
    "\n",
    "    model = SimpleRegularizedMLP(len(x_cols)*context_size, prediction_size).to(DEVICE)\n",
    "    learner = Learner(model=model, train_loader=train_loader, val_loader=valid_loader, verbose=False)\n",
    "    learner.fit(lr=learning_rate, epochs=epochs)\n",
    "\n",
    "    y_pred = learner.predict(test_loader, inverse_transform=target_scaler.inverse_transform)\n",
    "\n",
    "    y_pred.index.name = \"time\"\n",
    "    y_pred[\"model\"] = model.__class__.__name__\n",
    "    y_pred.set_index([\"model\"], append=True, inplace=True)\n",
    "    y_pred = y_pred.to_xarray()\n",
    "    scores = benchmark_scores.compute_scores(y_pred, y_test.to_xarray()[y_cols[0]], metrics=[\"mae\", \"rmse\", \"nse\", \"kge\"])\n",
    "\n",
    "    mean_scores = {s.upper(): round(float(scores[s].mean().values), 2) for s in scores.data_vars}\n",
    "    print(f\"ðŸ“Š RÃ©sultats pour window_size={context_size} -> {mean_scores}\")\n",
    "\n",
    "    # ðŸ”¹ Stocker les rÃ©sultats\n",
    "    results.append({\"ctx_size\": context_size, **mean_scores})\n",
    "\n",
    "# ðŸ”¹ Afficher le meilleur rÃ©sultat\n",
    "best = min(results, key=lambda x: x[\"RMSE\"])  # Choix basÃ© sur le RMSE le plus bas\n",
    "print(f\"\\nâœ… Meilleure fenÃªtre : {best[\"ctx_size\"]} avec RMSE={best[\"RMSE\"]}, MAE={best[\"MAE\"]}, NSE={best[\"NSE\"]}, KGE={best[\"KGE\"]}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14",
   "metadata": {},
   "source": [
    "- MLP raw tamsat: Meilleure fenÃªtre : 60 avec RMSE=141.604, MAE=73.313, RÂ²=0.887\n",
    "- MLP smooth 7d tamsat: Meilleure fenÃªtre : 50 avec RMSE=139.83, MAE=69.64, NSE=0.89, KGE=0.86\n",
    "- MLP smooth 7d imerg: Meilleure fenÃªtre : 55 avec RMSE=147.631, MAE=74.793, RÂ²=0.876\n",
    "- MLP: Meilleure fenÃªtre : 60 avec RMSE=156.864, MAE=78.964, RÂ²=0.861\n",
    "- CNN: Meilleure fenÃªtre : 30 avec RMSE=191.998, MAE=92.670, RÂ²=0.786\n",
    "- GRU: Meilleure fenÃªtre : 60 avec RMSE=212.033, MAE=108.959, RÂ²=0.746\n",
    "- Simple LSTM: Meilleure fenÃªtre : 90 avec RMSE=197.514, MAE=104.173, RÂ²=0.785"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ðŸ”¹ Fonction pour calculer le PBIAS\n",
    "def pbias(y_true, y_pred):\n",
    "    return 100 * np.sum(y_pred - y_true) / np.sum(y_true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "from ombs_senegal.benchmark_model import plot_prediction_comparison\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17",
   "metadata": {},
   "outputs": [],
   "source": [
    "_ = plot_prediction_comparison(\n",
    "    predicted=y_pred.sel(model=\"SimpleRegularizedMLP\"),\n",
    "    observed=y_test.to_xarray()[y_cols[0]],\n",
    "    mgb=x_test[\"dÃ©bit_mgb\"].to_xarray(),\n",
    "    scores=scores.sel(model=\"SimpleRegularizedMLP\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred.sel(model=\"SimpleRegularizedMLP\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "benchmark_scores = scores.to_array(\"score\", name=\"scores\")\n",
    "benchmark_results = xr.merge([\n",
    "    y_pred.to_array(\"forecast_horizon\", name=\"pred\"),\n",
    "    y_test.to_xarray().rename({\"dÃ©bit_insitu\": \"obs\"})[\"obs\"],\n",
    "    benchmark_scores\n",
    "    ])\n",
    "benchmark_results.to_netcdf(DATA_PATH/'mlp_with_tamsat.nc')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
