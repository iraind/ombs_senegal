{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TAMSAT Pertinence Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| default_exp tamsat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "from nbdev.showdoc import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "from pathlib import Path\n",
    "import xarray as xr\n",
    "import geopandas as gpd\n",
    "import hvplot.pandas\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from ombs_senegal.region import get_region_mask\n",
    "\n",
    "\n",
    "DATA_PATH = Path(\"../../data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TAMSAT Data preprocessing\n",
    "\n",
    "This section preprocesses TAMSAT rainfall data. First we will load and mask TAMSAT data over the region of interest\n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| skip_export\n",
    "tamsat = xr.open_dataset(DATA_PATH/\"01-tamsatDaily.v3.1-20100101-20250531-20250603_-16.85_-6.05_10.15_18.95.nc\")\n",
    "roi_gdf = gpd.read_file(DATA_PATH/\"point_ajustement/sub_poly.shp\")\n",
    "mask = get_region_mask(tamsat, roi_gdf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| skip_export\n",
    "roi_tamsat = tamsat.where(mask)\n",
    "roi_tamsat = roi_tamsat.sel(time=slice(None, \"2024-12-31\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since we're interested in the total rainfall across the basin rather than its spatial distribution, we'll sum up all rainfall values within the basin area. We'll save this aggregated data to avoid repeating the preprocessing steps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| skip_export\n",
    "daily_total = roi_tamsat.sum([\"lat\", \"lon\"])\n",
    "daily_total.to_netcdf(DATA_PATH/\"tamsat_sub_poly_daily_total.nc\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TAMSAT estimate to in situ correlation\n",
    "\n",
    "We will analyze the correlation between TAMSAT rainfall estimates and observed river discharge (débit).\n",
    "To reduce noise and identify long-term patterns, we'll aggregate the data annually. This will help us:\n",
    "1. Evaluate how well TAMSAT rainfall estimates correspond to actual river flow\n",
    "2. Assess the potential effectiveness of using TAMSAT data in our benchmark model\n",
    "3. Account for seasonal patterns and lag effects between rainfall and discharge\n",
    "\n",
    "The correlation analysis will provide insights into whether TAMSAT data can be a reliable predictor for river discharge in our study area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| skip_export\n",
    "insitu_df = pd.read_csv(\n",
    "    DATA_PATH/'data_cumul.csv', \n",
    "    sep=';', \n",
    "    usecols=['time', 'débit_insitu'], \n",
    "    index_col='time',\n",
    "    converters={\"time\": pd.to_datetime}\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tamsat_daily_total = xr.load_dataset(DATA_PATH/\"tamsat_sub_poly_daily_total.nc\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| skip_export\n",
    "combined_df = pd.merge(insitu_df, tamsat_daily_total[\"rfe\"].to_dataframe(), left_index=True, right_index=True)\n",
    "yearly_df = combined_df.resample(\"YS\").sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| skip_export\n",
    "plt.figure(figsize=(7,6))\n",
    "plt.scatter(yearly_df['débit_insitu'], yearly_df['rfe'])\n",
    "\n",
    "# Add year labels to each point\n",
    "for idx, row in yearly_df.iterrows():\n",
    "    plt.annotate(idx.year, (row['débit_insitu'], row['rfe']), xytext=(5,5), textcoords='offset points')\n",
    "\n",
    "plt.xlabel('Débit in-situ')\n",
    "plt.ylabel('Rainfall Estimate (mm)')\n",
    "plt.title('Débit vs Rainfall')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, in the above graph we can see that, even if there are some outliers such as 2020, there is a big correlation between the rainfall estimate and the river flow.\n",
    "\n",
    "We will now take a closer look by plotting the smoothed and normalized daily data.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| skip_export\n",
    "from ombs_senegal.benchmark_model import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| export\n",
    "def smooth(df, window=7, missing_values=0):\n",
    "    smoothed_df = df.copy()\n",
    "    smoothed_df = smoothed_df.rolling(window=window).sum()\n",
    "    return smoothed_df.fillna(missing_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| skip_export\n",
    "processed_df = combined_df.copy()\n",
    "processed_df[\"rfe\"] = smooth(combined_df[\"rfe\"], window=7)\n",
    "normalized_df = normalize(processed_df)\n",
    "\n",
    "normalized_df.hvplot.line()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Benchmark with TAMSAT \n",
    "\n",
    "Based on the strong correlation observed between TAMSAT rainfall estimates and river flow, we will now evaluate the benchmark model using TAMSAT data. We will conduct two analyses:\n",
    "1. Using only TAMSAT rainfall estimates and MGB water flow predictions as input features\n",
    "2. Using all available parameters (TAMSAT rainfall, MGB flow, and other variables) as input features\n",
    "\n",
    "Similar to our previous analysis with IMERG data, we will:\n",
    "- Test different time window sizes to capture temporal patterns\n",
    "- Evaluate multiple polynomial degrees to model non-linear relationships\n",
    "- Compare model performance using standard metrics (MSE, MAE) and visual analysis\n",
    "\n",
    "This will allow us to:\n",
    "- Assess TAMSAT's effectiveness as a predictor\n",
    "- Compare results with the IMERG-based models\n",
    "- Determine optimal model parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from ombs_senegal.benchmark_model import FeatureGenerator, SimpleRegressionModel\n",
    "\n",
    "from ombs_senegal.benchmark_model import plot_benchmark_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| skip_export\n",
    "df = pd.read_csv(\n",
    "    DATA_PATH/'data_cumul.csv', \n",
    "    sep=';', \n",
    "    usecols=['time', 'P_cumul_7j', 'débit_insitu', 'débit_mgb'], \n",
    "    index_col='time',\n",
    "    converters={\"time\": pd.to_datetime}\n",
    "    )\n",
    "\n",
    "tamsat_daily_total = xr.load_dataset(DATA_PATH/\"tamsat_sub_poly_daily_total.nc\")\n",
    "\n",
    "data = pd.merge(df, tamsat_daily_total[\"rfe\"].to_dataframe(), left_index=True, right_index=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| skip_export\n",
    "train_mask = df.index < '2019-01-01'\n",
    "x_col, y_col = [\"débit_mgb\", \"rfe\"], ['débit_insitu']\n",
    "\n",
    "data[\"rfe\"] = smooth(data[\"rfe\"], window=7)\n",
    "normalized_data = normalize(data[x_col])\n",
    "normalized_data[y_col[0]] = data[y_col[0]]\n",
    "\n",
    "train = normalized_data[train_mask]\n",
    "valid = normalized_data[~train_mask]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| skip_export\n",
    "\n",
    "predictions = []\n",
    "scores = []\n",
    "for degree in range(1, 4):\n",
    "    for window in range(10, 61, 10):\n",
    "        feature_generator = FeatureGenerator(window, degree)        \n",
    "        train_x, train_y = feature_generator.generate(train, x_col, y_col)\n",
    "        valid_x, valid_y = feature_generator.generate(valid, x_col, y_col)\n",
    "\n",
    "        model = SimpleRegressionModel()\n",
    "        model.fit(train_x, train_y)\n",
    "        pred = model.predict(valid_x)\n",
    "        predictions.append({'degree': degree, 'window': window, 'time': valid_x.index.values, 'prediction': pred.values,})\n",
    "\n",
    "        mse = round(mean_squared_error(valid_y, pred), 1)\n",
    "        mae = round(mean_absolute_error(valid_y, pred), 1)\n",
    "        # print(f\"Degree: {degree}, Window: {window}, MSE: {mse}, MAE: {mae}\")\n",
    "        scores.append({'degree': degree, 'window': window, 'mse': mse, 'mae': mae})\n",
    "\n",
    "scores = pd.DataFrame(scores).set_index(['degree', 'window']).sort_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set limits to zoom in on specific range of scores\n",
    "plot_benchmark_scores(scores, xlim=(None, 110), ylim=(None, 50000))\n",
    "  # Adjust y-axis limits to focus on MSE between 19000-21000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With IMERG: Degree: 1, Window: 50, MSE: 19955.6, MAE: 83.0\n",
    "\n",
    "Without IMERG: Degree: 2, Window: 50, MSE: 20931.7, MAE: 78.3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| skip_export\n",
    "obs_ds= valid[y_col].to_xarray()\n",
    "\n",
    "predictions = pd.DataFrame(predictions).explode(['prediction', 'time'])\n",
    "predictions = predictions.set_index(['degree', 'window', 'time']).sort_index()\n",
    "predictions_ds = predictions.to_xarray()\n",
    "\n",
    "scores_ds = scores.to_xarray()\n",
    "\n",
    "results_ds = predictions_ds.merge(scores_ds)\n",
    "results_ds = results_ds.merge(obs_ds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| skip_export\n",
    "results_ds.sel(degree=2, window=60, drop=True)[[\"prediction\", \"débit_insitu\"]].to_dataframe().plot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#| hide\n",
    "import nbdev; nbdev.nbdev_export()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
