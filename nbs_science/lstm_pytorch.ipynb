{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0",
   "metadata": {},
   "source": [
    "# L'objectif est d'expÃ©rimenter diffÃ©rentes tailles de fenÃªtres temporelles (window_size) pour trouver celle qui donne les meilleures performances.\n",
    "\n",
    "## MÃ©thodologie\n",
    "* DÃ©finir une liste de tailles de fenÃªtres (window_size) Ã  tester, par exemple [30, 60, 90, 120].\n",
    "* CrÃ©er des sÃ©quences avec chaque window_size et un prediction_size fixe.\\\\\n",
    "* EntraÃ®ner le modÃ¨le LSTM sur chaque fenÃªtre.\n",
    "* Ã‰valuer les performances avec RMSE, MAE et RÂ².\n",
    "* Comparer les performances pour choisir la meilleure fenÃªtre.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1",
   "metadata": {},
   "source": [
    "## Preguntas\n",
    "\n",
    "- Â¿Por quÃ©, si la validation loss oscila tanto, no paras el entrenamiento antes?\n",
    "- Â¿Que es RobustNormalization?\n",
    "- AÃ±adir tensor board para seguir el entrenamiento\n",
    "- Quizas no sÃ©a relevante para el entrenamiento y la predicciÃ³n del modelo pero Â¿el hecho de que robust scaler haga que haya lluvia negativa no va a afectar? Quizas habrÃ­a que revisarlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q plotly tensorflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip3 install -q torch --index-url https://download.pytorch.org/whl/cpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import plotly.graph_objects as go\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "DATA_PATH = Path(\"../../data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv(DATA_PATH / \"data_cumul.csv\", delimiter=\";\", parse_dates=True, index_col=\"time\")\n",
    "data = data[[\"P_cumul_7j\",\"dÃ©bit_mgb\",\"dÃ©bit_insitu\"]]\n",
    "data = data[\"2012-01-01\":]\n",
    "data[\"mois\"] = data.index.month\n",
    "data.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pd.DataFrame(features_scaled, index=data.index, columns=['P_cumul_7j','dÃ©bit_mgb',\"mois\"])[\"P_cumul_7j\"].plot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8",
   "metadata": {},
   "source": [
    "#### Data preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import RobustScaler\n",
    "# Mise Ã  l'Ã©chelle avec RobustScaler\n",
    "scaler_features = RobustScaler()\n",
    "scaler_target = RobustScaler()\n",
    "#features =data[[\"dÃ©bit_mgb_imerg\",\"Q1_IMERG\",\"Q2_IMERG\",\"Q3_IMERG\",\"mgb_wse_imerg\"]].values\n",
    "features =data[['P_cumul_7j','dÃ©bit_mgb',\"mois\"]].values\n",
    "#features=data[[\"dÃ©bit_mgb_imerg\",\"Q1_IMERG\",\"Q2_IMERG\",\"Q3_IMERG\",\"mgb_wse_imerg\"]].values\n",
    "target =data['dÃ©bit_insitu'].values.reshape(-1, 1)\n",
    "targets =data['dÃ©bit_mgb'].values.reshape(-1, 1)\n",
    "features_scaled = scaler_features.fit_transform(features)\n",
    "target_scaled = scaler_target.fit_transform(target)\n",
    "targets_scaled = scaler_target.fit_transform(targets)\n",
    "\n",
    "# Diviser les donnÃ©es en train, validation et test\n",
    "train_size = int(len(features_scaled) * 0.6)\n",
    "val_size = int(len(features_scaled) * 0.2)\n",
    "test_size = len(features_scaled) - train_size - val_size\n",
    "\n",
    "train_features = features_scaled[:train_size]\n",
    "train_target = target_scaled[:train_size]\n",
    "\n",
    "val_features = features_scaled[train_size:train_size + val_size]\n",
    "val_target = target_scaled[train_size:train_size + val_size]\n",
    "\n",
    "test_features = features_scaled[train_size + val_size:]\n",
    "test_target = target_scaled[train_size + val_size:]\n",
    "test_targets = targets_scaled[train_size + val_size:]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10",
   "metadata": {},
   "source": [
    "#### Model definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "11",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class LSTMModel(nn.Module):\n",
    "    def __init__(self, input_size, window_size, prediction_size):\n",
    "        super().__init__()\n",
    "        self.bilstm1 = nn.LSTM(input_size, 256, bidirectional=True, batch_first=True)\n",
    "        self.dropout1 = nn.Dropout(0.2)\n",
    "        self.bilstm2 = nn.LSTM(512, 128, bidirectional=True, batch_first=True)\n",
    "        self.dropout2 = nn.Dropout(0.2)\n",
    "        self.lstm = nn.LSTM(256, 64, batch_first=True)\n",
    "        self.dropout3 = nn.Dropout(0.2)\n",
    "        self.dense1 = nn.Linear(64, 128)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.dropout4 = nn.Dropout(0.2)\n",
    "        self.dense2 = nn.Linear(128, prediction_size)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x, _ = self.bilstm1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x, _ = self.bilstm2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x, _ = self.lstm(x)\n",
    "        x = x[:, -1, :]  # Take last output\n",
    "        x = self.dropout3(x)\n",
    "        x = self.dense1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.dropout4(x)\n",
    "        x = self.dense2(x)\n",
    "        return x\n",
    "    # ðŸ”¹ Initialisation du modÃ¨le\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12",
   "metadata": {},
   "source": [
    "#### Learner definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13",
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "class Learner:\n",
    "    def __init__(self,\n",
    "                 model: nn.Module, # model to train\n",
    "                 train_loader: DataLoader, # data loader for training data\n",
    "                 val_loader: DataLoader, # data loader for validation data\n",
    "                 criterion: nn.Module = nn.MSELoss(), # loss function to optimize\n",
    "                 optimizer: torch.optim.Optimizer = torch.optim.Adam, # optimizer class to use for training\n",
    "                 log_dir: str = 'runs', # directory to save tensorboard logs\n",
    "                 ) -> None:\n",
    "        self.model = model\n",
    "        self.train_loader = train_loader\n",
    "        self.val_loader = val_loader\n",
    "        self.criterion = criterion\n",
    "        self.optimizer = optimizer\n",
    "        self.writer = SummaryWriter(log_dir)\n",
    "\n",
    "    def fit(self, lr=0.001, epochs=10):\n",
    "        optimizer = self.optimizer(self.model.parameters(), lr=lr)\n",
    "        for epoch in tqdm(range(epochs), desc='Training epochs'):\n",
    "            # Training\n",
    "            self.model.train()\n",
    "            epoch_loss = 0\n",
    "            for batch_X, batch_y in self.train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                outputs = self.model(batch_X)\n",
    "                loss = self.criterion(outputs, batch_y.squeeze())\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "                epoch_loss += loss.item()\n",
    "\n",
    "            avg_train_loss = epoch_loss/len(self.train_loader)\n",
    "            self.writer.add_scalar('Training Loss/epoch', avg_train_loss, epoch)\n",
    "\n",
    "            # Validation\n",
    "            self.model.eval()\n",
    "            val_loss = 0\n",
    "            with torch.no_grad():\n",
    "                for batch_X, batch_y in self.val_loader:\n",
    "                    val_outputs = self.model(batch_X)\n",
    "                    val_loss += self.criterion(val_outputs, batch_y.squeeze()).item()\n",
    "            \n",
    "            avg_val_loss = val_loss/len(self.val_loader)\n",
    "            self.writer.add_scalar('Validation Loss/epoch', avg_val_loss, epoch)\n",
    "            \n",
    "            # print(f'Epoch {epoch+1}, Loss: {avg_train_loss:.4f}, Val Loss: {avg_val_loss:.4f}')\n",
    "    \n",
    "    def predict(self, dl: DataLoader):\n",
    "        self.model.eval()\n",
    "        predictions = []\n",
    "        targets = []\n",
    "        with torch.no_grad():\n",
    "            for batch_X, batch_y in dl:\n",
    "                batch_pred = self.model(batch_X).cpu().numpy()\n",
    "                predictions.append(batch_pred)\n",
    "        \n",
    "        return np.vstack(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14",
   "metadata": {},
   "outputs": [],
   "source": [
    "model = LSTMModel(X_train.shape[2], window_size, prediction_size).to(device)\n",
    "learner = Learner(model, train_loader, val_loader)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15",
   "metadata": {},
   "outputs": [],
   "source": [
    "learner.lr_find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# ðŸ”¹ DÃ©finition de la fonction pour crÃ©er des sÃ©quences\n",
    "def create_sequences(features, target, window_size, prediction_size):\n",
    "    X, y = [], []\n",
    "    for i in range(len(features) - window_size - prediction_size):\n",
    "        X.append(features[i:i+window_size])\n",
    "        y.append(target[i+window_size:i+window_size+prediction_size])\n",
    "    return torch.FloatTensor(X).to(device), torch.FloatTensor(y).to(device)\n",
    "\n",
    "# ðŸ”¹ Listes des tailles de fenÃªtres Ã  tester\n",
    "window_sizes = [10,]#20,30, 60, 90, 120]\n",
    "prediction_size = 10  # Fixe (peut Ãªtre ajustÃ©)\n",
    "batch_size = 48  # DÃ©finition de la taille des batchs\n",
    "results = []\n",
    "models = []\n",
    "\n",
    "# ðŸ”¹ Boucle sur diffÃ©rentes tailles de fenÃªtres\n",
    "for window_size in window_sizes:\n",
    "    print(f\"\\nðŸŸ¢ Test avec window_size = {window_size}\")\n",
    "\n",
    "    # CrÃ©ation des sÃ©quences\n",
    "    X_train, y_train = create_sequences(train_features, train_target, window_size, prediction_size)\n",
    "    X_val, y_val = create_sequences(val_features, val_target, window_size, prediction_size)\n",
    "    X_test, y_test = create_sequences(test_features, test_target, window_size, prediction_size)\n",
    "\n",
    "    # CrÃ©ation des DataLoaders\n",
    "    train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(TensorDataset(X_val, y_val), batch_size=batch_size)\n",
    "    test_loader = DataLoader(TensorDataset(X_test, y_test), batch_size=batch_size)\n",
    "    break\n",
    "\n",
    "    # ðŸ”¹ VÃ©rification des dimensions\n",
    "    model = LSTMModel(X_train.shape[2], window_size, prediction_size).to(device)\n",
    "    learner = Learner(model, train_loader, val_loader)\n",
    "    learner = Learner(model, train_loader, val_loader)\n",
    "    learner.fit(lr=0.0001, epochs=20)\n",
    "\n",
    "    y_pred = learner.predict(test_loader)\n",
    "\n",
    "    # ðŸ”¹ Inversion de l'Ã©chelle si nÃ©cessaire\n",
    "    y_test_rescaled = scaler_target.inverse_transform(y_test.reshape(-1, prediction_size))\n",
    "    y_pred_rescaled = scaler_target.inverse_transform(y_pred.reshape(-1, prediction_size))\n",
    "\n",
    "    # ðŸ”¹ Calcul des mÃ©triques (moyenne sur l'horizon de 10 jours)\n",
    "    rmse = np.mean([np.sqrt(mean_squared_error(y_test_rescaled[:, t], y_pred_rescaled[:, t])) for t in range(prediction_size)])\n",
    "    mae = np.mean([mean_absolute_error(y_test_rescaled[:, t], y_pred_rescaled[:, t]) for t in range(prediction_size)])\n",
    "    r2 = np.mean([r2_score(y_test_rescaled[:, t], y_pred_rescaled[:, t]) for t in range(prediction_size)])\n",
    "\n",
    "    print(f\"ðŸ“Š RÃ©sultats pour window_size={window_size} -> RMSE: {rmse:.3f}, MAE: {mae:.3f}, RÂ²: {r2:.3f}\")\n",
    "\n",
    "    # ðŸ”¹ Stocker les rÃ©sultats\n",
    "    results.append((window_size, rmse, mae, r2))\n",
    "\n",
    "# ðŸ”¹ Afficher le meilleur rÃ©sultat\n",
    "best_window = min(results, key=lambda x: x[1])  # Choix basÃ© sur le RMSE le plus bas\n",
    "print(f\"\\nâœ… Meilleure fenÃªtre : {best_window[0]} avec RMSE={best_window[1]:.3f}, MAE={best_window[2]:.3f}, RÂ²={best_window[3]:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17",
   "metadata": {},
   "source": [
    "#### Model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from torch.utils.data import TensorDataset, DataLoader\n",
    "\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "# ðŸ”¹ DÃ©finition de la fonction pour crÃ©er des sÃ©quences\n",
    "def create_sequences(features, target, window_size, prediction_size):\n",
    "    X, y = [], []\n",
    "    for i in range(len(features) - window_size - prediction_size):\n",
    "        X.append(features[i:i+window_size])\n",
    "        y.append(target[i+window_size:i+window_size+prediction_size])\n",
    "    return torch.FloatTensor(X).to(device), torch.FloatTensor(y).to(device)\n",
    "\n",
    "# ðŸ”¹ Listes des tailles de fenÃªtres Ã  tester\n",
    "window_sizes = [10,]#20,30, 60, 90, 120]\n",
    "prediction_size = 10  # Fixe (peut Ãªtre ajustÃ©)\n",
    "batch_size = 48  # DÃ©finition de la taille des batchs\n",
    "results = []\n",
    "models = []\n",
    "\n",
    "# ðŸ”¹ Boucle sur diffÃ©rentes tailles de fenÃªtres\n",
    "for window_size in window_sizes:\n",
    "    print(f\"\\nðŸŸ¢ Test avec window_size = {window_size}\")\n",
    "\n",
    "    # CrÃ©ation des sÃ©quences\n",
    "    X_train, y_train = create_sequences(train_features, train_target, window_size, prediction_size)\n",
    "    X_val, y_val = create_sequences(val_features, val_target, window_size, prediction_size)\n",
    "    X_test, y_test = create_sequences(test_features, test_target, window_size, prediction_size)\n",
    "\n",
    "    # CrÃ©ation des DataLoaders\n",
    "    train_loader = DataLoader(TensorDataset(X_train, y_train), batch_size=batch_size, shuffle=True)\n",
    "    val_loader = DataLoader(TensorDataset(X_val, y_val), batch_size=batch_size)\n",
    "    test_loader = DataLoader(TensorDataset(X_test, y_test), batch_size=batch_size)\n",
    "\n",
    "    # ðŸ”¹ VÃ©rification des dimensions\n",
    "    model = LSTMModel(X_train.shape[2], window_size, prediction_size).to(device)\n",
    "    learner = Learner(model, train_loader, val_loader)\n",
    "    learner.fit(lr=0.0001, epochs=20)\n",
    "\n",
    "    y_pred = learner.predict(test_loader)\n",
    "\n",
    "    # ðŸ”¹ Inversion de l'Ã©chelle si nÃ©cessaire\n",
    "    y_test_rescaled = scaler_target.inverse_transform(y_test.reshape(-1, prediction_size))\n",
    "    y_pred_rescaled = scaler_target.inverse_transform(y_pred.reshape(-1, prediction_size))\n",
    "\n",
    "    # ðŸ”¹ Calcul des mÃ©triques (moyenne sur l'horizon de 10 jours)\n",
    "    rmse = np.mean([np.sqrt(mean_squared_error(y_test_rescaled[:, t], y_pred_rescaled[:, t])) for t in range(prediction_size)])\n",
    "    mae = np.mean([mean_absolute_error(y_test_rescaled[:, t], y_pred_rescaled[:, t]) for t in range(prediction_size)])\n",
    "    r2 = np.mean([r2_score(y_test_rescaled[:, t], y_pred_rescaled[:, t]) for t in range(prediction_size)])\n",
    "\n",
    "    print(f\"ðŸ“Š RÃ©sultats pour window_size={window_size} -> RMSE: {rmse:.3f}, MAE: {mae:.3f}, RÂ²: {r2:.3f}\")\n",
    "\n",
    "    # ðŸ”¹ Stocker les rÃ©sultats\n",
    "    results.append((window_size, rmse, mae, r2))\n",
    "\n",
    "# ðŸ”¹ Afficher le meilleur rÃ©sultat\n",
    "best_window = min(results, key=lambda x: x[1])  # Choix basÃ© sur le RMSE le plus bas\n",
    "print(f\"\\nâœ… Meilleure fenÃªtre : {best_window[0]} avec RMSE={best_window[1]:.3f}, MAE={best_window[2]:.3f}, RÂ²={best_window[3]:.3f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "20",
   "metadata": {},
   "source": [
    "#### Assesment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Extraire les valeurs\n",
    "window_sizes, rmse_values, mae_values, r2_values = zip(*results)\n",
    "\n",
    "# Tracer l'Ã©volution du RMSE\n",
    "plt.figure(figsize=(8, 5))\n",
    "plt.plot(window_sizes, rmse_values, marker='o', linestyle='-', color='b', label=\"RMSE\")\n",
    "plt.xlabel(\"Taille de la fenÃªtre temporelle\")\n",
    "plt.ylabel(\"Erreur (RMSE)\")\n",
    "plt.title(\"Impact de la fenÃªtre temporelle sur la prÃ©cision\")\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22",
   "metadata": {},
   "source": [
    "# Conclusions\n",
    "* Si window_size est trop petit, le modÃ¨le manque de contexte et ne capture pas bien les tendances.\n",
    "* Si window_size est trop grand, il risque dâ€™avoir trop dâ€™informations inutiles et de perdre en gÃ©nÃ©ralisation.\n",
    "* Le test permet de trouver un compromis optimal pour minimiser lerreur (RMSE, MAE) et maximiser RÂ²."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23",
   "metadata": {},
   "source": [
    "# EnchaÃ®ner avec la meilleure fenÃªtre temporelle\n",
    "* AprÃ¨s avoir trouvÃ© la meilleure taille de fenÃªtre temporelle (best_window), l'idÃ©e est de :\n",
    "\n",
    "* RÃ©entraÃ®ner le modÃ¨le LSTM avec cette meilleure window_size.\n",
    "* Effectuer la prÃ©diction finale sur X_test.\n",
    "* Analyser l'erreur moyenne par jour de prÃ©diction pour voir comment elle Ã©volue sur les 10 jours."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.losses import Huber\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense, Dropout, Bidirectional\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# ðŸ”¹ DÃ©finition de la meilleure fenÃªtre temporelle (aprÃ¨s test)\n",
    "best_window_size = 10 #best_window[0]  # RÃ©cupÃ©rer la taille optimale trouvÃ©e prÃ©cÃ©demment\n",
    "prediction_size = 10  # Horizon de prÃ©vision\n",
    "\n",
    "# ðŸ”¹ CrÃ©ation des sÃ©quences avec la meilleure fenÃªtre\n",
    "X_train, y_train = create_sequences(train_features, train_target, best_window_size, prediction_size)\n",
    "X_val, y_val = create_sequences(val_features, val_target, best_window_size, prediction_size)\n",
    "X_test, y_test = create_sequences(test_features, test_target, best_window_size, prediction_size)\n",
    "x_test, y_test_mgb = create_sequences(test_features, test_targets, best_window_size, prediction_size)\n",
    "\n",
    "# ðŸ”¹ VÃ©rification des dimensions\n",
    "print(f\"X_train: {X_train.shape}, y_train: {y_train.shape}\")\n",
    "\n",
    "# ðŸ”¹ RedÃ©finition du modÃ¨le LSTM\n",
    "model = Sequential([\n",
    "    Bidirectional(LSTM(256, return_sequences=True, input_shape=(best_window_size, X_train.shape[2]))),\n",
    "    Dropout(0.2),\n",
    "    Bidirectional(LSTM(128, return_sequences=True)),\n",
    "    Dropout(0.2),\n",
    "    LSTM(64, return_sequences=False),\n",
    "    Dropout(0.2),\n",
    "    Dense(128, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "    Dense(prediction_size)\n",
    "])\n",
    "\n",
    "# ðŸ”¹ Compilation et entraÃ®nement\n",
    "model.compile(optimizer='adam',loss=Huber(), metrics=['mae'])\n",
    "history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=20, batch_size=32, verbose=1)\n",
    "\n",
    "# ðŸ”¹ PrÃ©diction finale avec la meilleure fenÃªtre\n",
    "y_pred = model.predict(X_test)\n",
    "\n",
    "# ðŸ”¹ Inversion de l'Ã©chelle des prÃ©dictions et des valeurs rÃ©elles\n",
    "y_test_rescaled = scaler_target.inverse_transform(y_test.reshape(-1, prediction_size))\n",
    "y_pred_rescaled = scaler_target.inverse_transform(y_pred.reshape(-1, prediction_size))\n",
    "y_test_mgb_rescaled = scaler_target.inverse_transform(y_test_mgb.reshape(-1, prediction_size))\n",
    "\n",
    "# ðŸ”¹ Calcul des mÃ©triques de performance\n",
    "rmse = np.mean([np.sqrt(mean_squared_error(y_test_rescaled[:, t], y_pred_rescaled[:, t])) for t in range(prediction_size)])\n",
    "mae = np.mean([mean_absolute_error(y_test_rescaled[:, t], y_pred_rescaled[:, t]) for t in range(prediction_size)])\n",
    "r2 = np.mean([r2_score(y_test_rescaled[:, t], y_pred_rescaled[:, t]) for t in range(prediction_size)])\n",
    "\n",
    "print(f\"\\nðŸ“Š RÃ©sultats finaux avec window_size={best_window_size} -> RMSE: {rmse:.3f}, MAE: {mae:.3f}, RÂ²: {r2:.3f}\")\n",
    "\n",
    "# ðŸ”¹ Calcul de l'erreur absolue par jour de prÃ©diction\n",
    "error = np.abs(y_test_rescaled - y_pred_rescaled)\n",
    "\n",
    "# ðŸ”¹ Affichage de l'Ã©volution de l'erreur moyenne par jour de prÃ©diction\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.plot(np.mean(error, axis=0), marker='o', linestyle='dashed', color='red')\n",
    "\n",
    "plt.xlabel(\"Jour de prÃ©diction de (t+1 Ã  t+10)\")\n",
    "plt.ylabel(\"Erreur moyenne\")\n",
    "plt.title(\"Ã‰volution de l'erreur de prÃ©diction par jour\")\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# avec loss =Huber\n",
    "# ðŸ”¹ Reconstruction des sÃ©ries temporelles\n",
    "y_test_mgb_reconstructed = np.concatenate([y_test_mgb_rescaled[i] for i in range(y_test_mgb_rescaled.shape[0] - 1)], axis=0)\n",
    "y_test_reconstructed = np.concatenate([y_test_rescaled[i] for i in range(y_test_rescaled.shape[0] - 1)], axis=0)\n",
    "y_pred_reconstructed = np.concatenate([y_pred_rescaled[i] for i in range(y_pred_rescaled.shape[0] - 1)], axis=0)\n",
    "\n",
    "# ðŸ”¹ CrÃ©ation des graphiques\n",
    "fig, axes = plt.subplots(5, 2, figsize=(14, 10), sharex=True, sharey=True)\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(prediction_size):  # Pour chaque jour de l'horizon de prÃ©diction\n",
    "    # SÃ©lectionner les valeurs correspondantes aux sÃ©quences\n",
    "    y_true = y_test_reconstructed[i::prediction_size]  # Prend les vraies valeurs pour le jour i\n",
    "    y_pred = y_pred_reconstructed[i::prediction_size]  # Prend les prÃ©dictions pour le jour i\n",
    "    y_test_mgb =y_test_mgb_reconstructed [i::prediction_size]  # Prend les prÃ©dictions pour le jour i\n",
    "    # Calcul des mÃ©triques pour le jour i\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    correlation = np.corrcoef(y_true, y_pred)[0, 1]  # Coefficient de corrÃ©lation\n",
    "\n",
    "    # Tracer les courbes\n",
    "    axes[i].plot(y_true, label=\"Q_obs\", color='blue')\n",
    "    axes[i].plot(y_pred, label=\"Q_pred (prÃ©dit)\", color='red', linestyle='solid')\n",
    "    axes[i].plot(y_test_mgb, label=\"Q_mgb\", color='black')\n",
    "\n",
    "    # Ajouter la mÃ©trique dans le titre\n",
    "    axes[i].set_title(f\"Jour {i+1}\\nRMSE={rmse:.2f}, Corr={correlation:.2f}\")\n",
    "    \n",
    "    axes[i].grid()\n",
    "    axes[i].legend()\n",
    "\n",
    "plt.suptitle(\"Comparaison des valeurs rÃ©elles et prÃ©dites pour chaque jour de l'horizon de 10 jours\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# ðŸ”¹ Reconstruction des cibles et prÃ©dictions\n",
    "y_test_reconstructed = np.concatenate(y_test_rescaled[:-1], axis=0)\n",
    "y_pred_reconstructed = np.concatenate(y_pred_rescaled[:-1], axis=0)\n",
    "\n",
    "# ðŸ”¹ RÃ©cupÃ©ration du dÃ©bit_mgb dans les features (par exemple Ã  lâ€™indice 1)\n",
    "debit_mgb_index = 1\n",
    "\n",
    "# ðŸ”¹ On rÃ©cupÃ¨re la derniÃ¨re valeur de chaque sÃ©quence (alignÃ©e avec chaque prÃ©diction de 10 jours)\n",
    "debit_mgb_series = X_test[:-1, -1, debit_mgb_index].reshape(-1, 1)\n",
    "\n",
    "# ðŸ”¹ RÃ©pÃ©ter cette derniÃ¨re valeur pour chaque jour dâ€™horizon\n",
    "# Car chaque sÃ©quence gÃ©nÃ¨re 10 prÃ©dictions (horizon = 10)\n",
    "debit_mgb_reconstructed = np.tile(debit_mgb_series, (1, prediction_size)).reshape(-1)\n",
    "\n",
    "# ðŸ”¹ CrÃ©ation des sous-graphiques\n",
    "fig, axes = plt.subplots(5, 2, figsize=(14, 10), sharex=True, sharey=True)\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(prediction_size):\n",
    "    y_true = y_test_reconstructed[i::prediction_size]\n",
    "    y_pred = y_pred_reconstructed[i::prediction_size]\n",
    "    debit_mgb = debit_mgb_reconstructed[i::prediction_size]\n",
    "\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    corr = np.corrcoef(y_true, y_pred)[0, 1]\n",
    "\n",
    "    axes[i].plot(y_true, label=\"Y_test (rÃ©el)\", color='blue')\n",
    "    axes[i].plot(y_pred, label=\"Y_pred\", color='red', linestyle='solid')\n",
    "    axes[i].plot(debit_mgb, label=\"DÃ©bit MGB\", color='green', linestyle='dotted')\n",
    "\n",
    "    axes[i].set_title(f\"Jour {i+1} â€“ RMSE={rmse:.2f}, Corr={corr:.2f}\")\n",
    "    axes[i].legend()\n",
    "    axes[i].grid()\n",
    "\n",
    "plt.suptitle(\"Comparaison rÃ©elle / prÃ©dite avec DÃ©bit MGB sur les 10 jours dâ€™horizon\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "27",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Shape de X_test :\", x_rain.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exemple : on suppose que la variable pluie est la premiÃ¨re colonne (colonne 0) des features\n",
    "# Donc on rÃ©cupÃ¨re la mÃªme variable dans les X_test (mÃªme dÃ©coupage que y_test)\n",
    "rain_feature = test_features[:, 0]  # ou change le 0 selon la variable souhaitÃ©e\n",
    "\n",
    "# On recrÃ©e les sÃ©quences pour matcher les index\n",
    "y_rain = create_sequences(test_features, test_target, window_size=best_window_size, prediction_size=prediction_size)\n",
    "\n",
    "# Comme y_rain n'est pas utilisÃ© pour prÃ©dire mais pour visualiser, on peut extraire la mÃªme logique\n",
    "rain_reconstructed = np.concatenate([y_rain[i] for i in range(y_rain.shape[0] - 1)], axis=0)\n",
    "    # Tracer les courbes\n",
    "    axes[i].plot(y_true, label=\"Y_test (rÃ©el)\", color='blue')\n",
    "    axes[i].plot(y_pred, label=\"Y_pred (prÃ©dit)\", color='red', linestyle='solid')\n",
    "    axes[i].plot(rain_reconstructed[i::prediction_size], label=\"Pluie (entrÃ©e)\", color='green', linestyle='dotted')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "# avec loss =Huber\n",
    "# ðŸ”¹ Reconstruction des sÃ©ries temporelles\n",
    "y_test_reconstructed = np.concatenate([y_test_rescaled[i] for i in range(y_test_rescaled.shape[0] - 1)], axis=0)\n",
    "y_pred_reconstructed = np.concatenate([y_pred_rescaled[i] for i in range(y_pred_rescaled.shape[0] - 1)], axis=0)\n",
    "\n",
    "# ðŸ”¹ CrÃ©ation des graphiques\n",
    "fig, axes = plt.subplots(5, 2, figsize=(14, 10), sharex=True, sharey=True)\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(prediction_size):  # Pour chaque jour de l'horizon de prÃ©diction\n",
    "    # SÃ©lectionner les valeurs correspondantes aux sÃ©quences\n",
    "    y_true = y_test_reconstructed[i::prediction_size]  # Prend les vraies valeurs pour le jour i\n",
    "    y_pred = y_pred_reconstructed[i::prediction_size]  # Prend les prÃ©dictions pour le jour i\n",
    "    \n",
    "    # Calcul des mÃ©triques pour le jour i\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    correlation = np.corrcoef(y_true, y_pred)[0, 1]  # Coefficient de corrÃ©lation\n",
    "\n",
    "    # Tracer les courbes\n",
    "    axes[i].plot(y_true, label=\"Y_test (rÃ©el)\", color='blue')\n",
    "    axes[i].plot(y_pred, label=\"Y_pred (prÃ©dit)\", color='red', linestyle='solid')\n",
    "\n",
    "    # Ajouter la mÃ©trique dans le titre\n",
    "    axes[i].set_title(f\"Jour {i+1}\\nRMSE={rmse:.2f}, Corr={correlation:.2f}\")\n",
    "    \n",
    "    axes[i].grid()\n",
    "    axes[i].legend()\n",
    "\n",
    "plt.suptitle(\"Comparaison des valeurs rÃ©elles et prÃ©dites pour chaque jour de l'horizon de 10 jours\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# ðŸ”¹ Reconstruction des sÃ©ries temporelles\n",
    "y_test_reconstructed = np.concatenate([y_test_rescaled[i] for i in range(y_test_rescaled.shape[0] - 1)], axis=0)\n",
    "y_pred_reconstructed = np.concatenate([y_pred_rescaled[i] for i in range(y_pred_rescaled.shape[0] - 1)], axis=0)\n",
    "\n",
    "# ðŸ”¹ CrÃ©ation des graphiques\n",
    "fig, axes = plt.subplots(5, 2, figsize=(14, 10), sharex=True, sharey=True)\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(prediction_size):  # Pour chaque jour de l'horizon de prÃ©diction\n",
    "    # SÃ©lectionner les valeurs correspondantes aux sÃ©quences\n",
    "    y_true = y_test_reconstructed[i::prediction_size]  # Prend les vraies valeurs pour le jour i\n",
    "    y_pred = y_pred_reconstructed[i::prediction_size]  # Prend les prÃ©dictions pour le jour i\n",
    "    \n",
    "    # Calcul des mÃ©triques pour le jour i\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    correlation = np.corrcoef(y_true, y_pred)[0, 1]  # Coefficient de corrÃ©lation\n",
    "\n",
    "    # Tracer les courbes\n",
    "    axes[i].plot(y_true, label=\"Y_test (rÃ©el)\", color='blue')\n",
    "    axes[i].plot(y_pred, label=\"Y_pred (prÃ©dit)\", color='red', linestyle='solid')\n",
    "\n",
    "    # Ajouter la mÃ©trique dans le titre\n",
    "    axes[i].set_title(f\"Jour {i+1}\\nRMSE={rmse:.2f}, Corr={correlation:.2f}\")\n",
    "    \n",
    "    axes[i].grid()\n",
    "    axes[i].legend()\n",
    "\n",
    "plt.suptitle(\"Comparaison des valeurs rÃ©elles et prÃ©dites pour chaque jour de l'horizon de 10 jours\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# ðŸ”¹ Reconstruction des sÃ©ries temporelles\n",
    "y_test_reconstructed = np.concatenate([y_test_rescaled[i] for i in range(y_test_rescaled.shape[0] - 1)], axis=0)\n",
    "y_pred_reconstructed = np.concatenate([y_pred_rescaled[i] for i in range(y_pred_rescaled.shape[0] - 1)], axis=0)\n",
    "\n",
    "# ðŸ”¹ CrÃ©ation des graphiques\n",
    "fig, axes = plt.subplots(5, 2, figsize=(14, 10), sharex=True, sharey=True)\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(prediction_size):  # Pour chaque jour de l'horizon de prÃ©diction\n",
    "    # SÃ©lectionner les valeurs correspondantes aux sÃ©quences\n",
    "    y_true = y_test_reconstructed[i::prediction_size]  # Prend les vraies valeurs pour le jour i\n",
    "    y_pred = y_pred_reconstructed[i::prediction_size]  # Prend les prÃ©dictions pour le jour i\n",
    "    \n",
    "    # Calcul des mÃ©triques pour le jour i\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    correlation = np.corrcoef(y_true, y_pred)[0, 1]  # Coefficient de corrÃ©lation\n",
    "\n",
    "    # Tracer les courbes\n",
    "    axes[i].plot(y_true, label=\"Y_test (rÃ©el)\", color='blue')\n",
    "    axes[i].plot(y_pred, label=\"Y_pred (prÃ©dit)\", color='red', linestyle='solid')\n",
    "\n",
    "    # Ajouter la mÃ©trique dans le titre\n",
    "    axes[i].set_title(f\"Jour {i+1}\\nRMSE={rmse:.2f}, Corr={correlation:.2f}\")\n",
    "    \n",
    "    axes[i].grid()\n",
    "    axes[i].legend()\n",
    "\n",
    "plt.suptitle(\"Comparaison des valeurs rÃ©elles et prÃ©dites pour chaque jour de l'horizon de 10 jours\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# ðŸ”¹ Reconstruction des sÃ©ries temporelles\n",
    "y_test_reconstructed = np.concatenate([y_test_rescaled[i] for i in range(y_test_rescaled.shape[0] - 1)], axis=0)\n",
    "y_pred_reconstructed = np.concatenate([y_pred_rescaled[i] for i in range(y_pred_rescaled.shape[0] - 1)], axis=0)\n",
    "\n",
    "# ðŸ”¹ CrÃ©ation des graphiques\n",
    "fig, axes = plt.subplots(5, 2, figsize=(14, 10), sharex=True, sharey=True)\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(prediction_size):  # Pour chaque jour de l'horizon de prÃ©diction\n",
    "    # SÃ©lectionner les valeurs correspondantes aux sÃ©quences\n",
    "    y_true = y_test_reconstructed[i::prediction_size]  # Prend les vraies valeurs pour le jour i\n",
    "    y_pred = y_pred_reconstructed[i::prediction_size]  # Prend les prÃ©dictions pour le jour i\n",
    "    \n",
    "    # Calcul des mÃ©triques pour le jour i\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    correlation = np.corrcoef(y_true, y_pred)[0, 1]  # Coefficient de corrÃ©lation\n",
    "\n",
    "    # Tracer les courbes\n",
    "    axes[i].plot(y_true, label=\"Y_test (rÃ©el)\", color='blue')\n",
    "    axes[i].plot(y_pred, label=\"Y_pred (prÃ©dit)\", color='red', linestyle='solid')\n",
    "\n",
    "    # Ajouter la mÃ©trique dans le titre\n",
    "    axes[i].set_title(f\"Jour {i+1}\\nRMSE={rmse:.2f}, Corr={correlation:.2f}\")\n",
    "    \n",
    "    axes[i].grid()\n",
    "    axes[i].legend()\n",
    "\n",
    "plt.suptitle(\"Comparaison des valeurs rÃ©elles et prÃ©dites pour chaque jour de l'horizon de 10 jours\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# ðŸ”¹ Reconstruction des sÃ©ries temporelles\n",
    "y_test_reconstructed = np.concatenate([y_test_rescaled[i] for i in range(y_test_rescaled.shape[0] - 1)], axis=0)\n",
    "y_pred_reconstructed = np.concatenate([y_pred_rescaled[i] for i in range(y_pred_rescaled.shape[0] - 1)], axis=0)\n",
    "\n",
    "# ðŸ”¹ CrÃ©ation des graphiques\n",
    "fig, axes = plt.subplots(5, 2, figsize=(14, 10), sharex=True, sharey=True)\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(prediction_size):  # Pour chaque jour de l'horizon de prÃ©diction\n",
    "    # SÃ©lectionner les valeurs correspondantes aux sÃ©quences\n",
    "    y_true = y_test_reconstructed[i::prediction_size]  # Prend les vraies valeurs pour le jour i\n",
    "    y_pred = y_pred_reconstructed[i::prediction_size]  # Prend les prÃ©dictions pour le jour i\n",
    "    \n",
    "    # Calcul des mÃ©triques pour le jour i\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    correlation = np.corrcoef(y_true, y_pred)[0, 1]  # Coefficient de corrÃ©lation\n",
    "\n",
    "    # Tracer les courbes\n",
    "    axes[i].plot(y_true, label=\"Y_test (rÃ©el)\", color='blue')\n",
    "    axes[i].plot(y_pred, label=\"Y_pred (prÃ©dit)\", color='red', linestyle='solid')\n",
    "\n",
    "    # Ajouter la mÃ©trique dans le titre\n",
    "    axes[i].set_title(f\"Jour {i+1}\\nRMSE={rmse:.2f}, Corr={correlation:.2f}\")\n",
    "    \n",
    "    axes[i].grid()\n",
    "    axes[i].legend()\n",
    "\n",
    "plt.suptitle(\"Comparaison des valeurs rÃ©elles et prÃ©dites pour chaque jour de l'horizon de 10 jours\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# ðŸ”¹ Reconstruction des sÃ©ries temporelles\n",
    "y_test_reconstructed = np.concatenate([y_test_rescaled[i] for i in range(y_test_rescaled.shape[0] - 1)], axis=0)\n",
    "y_pred_reconstructed = np.concatenate([y_pred_rescaled[i] for i in range(y_pred_rescaled.shape[0] - 1)], axis=0)\n",
    "\n",
    "# ðŸ”¹ CrÃ©ation des graphiques\n",
    "fig, axes = plt.subplots(5, 2, figsize=(14, 10), sharex=True, sharey=True)\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(prediction_size):  # Pour chaque jour de l'horizon de prÃ©diction\n",
    "    # SÃ©lectionner les valeurs correspondantes aux sÃ©quences\n",
    "    y_true = y_test_reconstructed[i::prediction_size]  # Prend les vraies valeurs pour le jour i\n",
    "    y_pred = y_pred_reconstructed[i::prediction_size]  # Prend les prÃ©dictions pour le jour i\n",
    "    \n",
    "    # Calcul des mÃ©triques pour le jour i\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    correlation = np.corrcoef(y_true, y_pred)[0, 1]  # Coefficient de corrÃ©lation\n",
    "\n",
    "    # Tracer les courbes\n",
    "    axes[i].plot(y_true, label=\"Y_test (rÃ©el)\", color='blue')\n",
    "    axes[i].plot(y_pred, label=\"Y_pred (prÃ©dit)\", color='red', linestyle='solid')\n",
    "\n",
    "    # Ajouter la mÃ©trique dans le titre\n",
    "    axes[i].set_title(f\"Jour {i+1}\\nRMSE={rmse:.2f}, Corr={correlation:.2f}\")\n",
    "    \n",
    "    axes[i].grid()\n",
    "    axes[i].legend()\n",
    "\n",
    "plt.suptitle(\"Comparaison des valeurs rÃ©elles et prÃ©dites pour chaque jour de l'horizon de 10 jours\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# ðŸ”¹ Reconstruction des sÃ©ries temporelles\n",
    "y_test_reconstructed = np.concatenate([y_test_rescaled[i] for i in range(y_test_rescaled.shape[0] - 1)], axis=0)\n",
    "y_pred_reconstructed = np.concatenate([y_pred_rescaled[i] for i in range(y_pred_rescaled.shape[0] - 1)], axis=0)\n",
    "\n",
    "# ðŸ”¹ CrÃ©ation des graphiques\n",
    "fig, axes = plt.subplots(5, 2, figsize=(14, 10), sharex=True, sharey=True)\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(prediction_size):  # Pour chaque jour de l'horizon de prÃ©diction\n",
    "    # SÃ©lectionner les valeurs correspondantes aux sÃ©quences\n",
    "    y_true = y_test_reconstructed[i::prediction_size]  # Prend les vraies valeurs pour le jour i\n",
    "    y_pred = y_pred_reconstructed[i::prediction_size]  # Prend les prÃ©dictions pour le jour i\n",
    "    \n",
    "    # Calcul des mÃ©triques pour le jour i\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    correlation = np.corrcoef(y_true, y_pred)[0, 1]  # Coefficient de corrÃ©lation\n",
    "\n",
    "    # Tracer les courbes\n",
    "    axes[i].plot(y_true, label=\"Y_test (rÃ©el)\", color='blue')\n",
    "    axes[i].plot(y_pred, label=\"Y_pred (prÃ©dit)\", color='red', linestyle='solid')\n",
    "\n",
    "    # Ajouter la mÃ©trique dans le titre\n",
    "    axes[i].set_title(f\"Jour {i+1}\\nRMSE={rmse:.2f}, Corr={correlation:.2f}\")\n",
    "    \n",
    "    axes[i].grid()\n",
    "    axes[i].legend()\n",
    "\n",
    "plt.suptitle(\"Comparaison des valeurs rÃ©elles et prÃ©dites pour chaque jour de l'horizon de 10 jours\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# ðŸ”¹ Reconstruction des sÃ©ries temporelles\n",
    "y_test_reconstructed = np.concatenate([y_test_rescaled[i] for i in range(y_test_rescaled.shape[0] - 1)], axis=0)\n",
    "y_pred_reconstructed = np.concatenate([y_pred_rescaled[i] for i in range(y_pred_rescaled.shape[0] - 1)], axis=0)\n",
    "\n",
    "# ðŸ”¹ CrÃ©ation des graphiques\n",
    "fig, axes = plt.subplots(5, 2, figsize=(14, 10), sharex=True, sharey=True)\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(prediction_size):  # Pour chaque jour de l'horizon de prÃ©diction\n",
    "    # SÃ©lectionner les valeurs correspondantes aux sÃ©quences\n",
    "    y_true = y_test_reconstructed[i::prediction_size]  # Prend les vraies valeurs pour le jour i\n",
    "    y_pred = y_pred_reconstructed[i::prediction_size]  # Prend les prÃ©dictions pour le jour i\n",
    "    \n",
    "    # Calcul des mÃ©triques pour le jour i\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    correlation = np.corrcoef(y_true, y_pred)[0, 1]  # Coefficient de corrÃ©lation\n",
    "\n",
    "    # Tracer les courbes\n",
    "    axes[i].plot(y_true, label=\"Y_test (rÃ©el)\", color='blue')\n",
    "    axes[i].plot(y_pred, label=\"Y_pred (prÃ©dit)\", color='red', linestyle='solid')\n",
    "\n",
    "    # Ajouter la mÃ©trique dans le titre\n",
    "    axes[i].set_title(f\"Jour {i+1}\\nRMSE={rmse:.2f}, Corr={correlation:.2f}\")\n",
    "    \n",
    "    axes[i].grid()\n",
    "    axes[i].legend()\n",
    "\n",
    "plt.suptitle(\"Comparaison des valeurs rÃ©elles et prÃ©dites pour chaque jour de l'horizon de 10 jours\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# ðŸ”¹ Reconstruction des sÃ©ries temporelles\n",
    "y_test_reconstructed = np.concatenate([y_test_rescaled[i] for i in range(y_test_rescaled.shape[0] - 1)], axis=0)\n",
    "y_pred_reconstructed = np.concatenate([y_pred_rescaled[i] for i in range(y_pred_rescaled.shape[0] - 1)], axis=0)\n",
    "\n",
    "# ðŸ”¹ CrÃ©ation des sous-graphiques avec Plotly\n",
    "fig = go.Figure()\n",
    "\n",
    "for i in range(prediction_size):  # Pour chaque jour de l'horizon de prÃ©diction\n",
    "    # SÃ©lectionner les valeurs correspondantes aux sÃ©quences\n",
    "    y_true = y_test_reconstructed[i::prediction_size]  # Prend les vraies valeurs pour le jour i\n",
    "    y_pred = y_pred_reconstructed[i::prediction_size]  # Prend les prÃ©dictions pour le jour i\n",
    "    \n",
    "    # Calcul des mÃ©triques pour le jour i\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    correlation = np.corrcoef(y_true, y_pred)[0, 1]  # Coefficient de corrÃ©lation\n",
    "    \n",
    "    # Ajouter un sous-graphique pour chaque jour\n",
    "    fig.add_trace(go.Scatter(y=y_true, mode='lines', name=f'Y_test Jour {i+1}', line=dict(color='blue')))\n",
    "    fig.add_trace(go.Scatter(y=y_pred, mode='lines', name=f'Y_pred Jour {i+1}', line=dict(color='red', dash='dash')))\n",
    "    \n",
    "    # Ajouter des annotations\n",
    "    fig.add_annotation(\n",
    "        x=len(y_true) - 1, y=max(y_true),\n",
    "        text=f\"Jour {i+1}<br>RMSE={rmse:.2f}, Corr={correlation:.2f}\",\n",
    "        showarrow=False, font=dict(size=10)\n",
    "    )\n",
    "\n",
    "# Mise en forme du graphe\n",
    "fig.update_layout(\n",
    "    title=\"Comparaison des valeurs rÃ©elles et prÃ©dites pour chaque jour de l'horizon de 10 jours\",\n",
    "    xaxis_title=\"Temps\",\n",
    "    yaxis_title=\"Valeurs\",\n",
    "    template=\"plotly_white\"\n",
    ")\n",
    "\n",
    "# Afficher le graphique\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import plotly.graph_objects as go\n",
    "from plotly.subplots import make_subplots\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "def kge(sim, obs):\n",
    "    cc = np.corrcoef(sim, obs)[0, 1]\n",
    "    alpha = np.std(sim) / np.std(obs)\n",
    "    beta = np.mean(sim) / np.mean(obs)\n",
    "    return 1 - np.sqrt((cc - 1)**2 + (alpha - 1)**2 + (beta - 1)**2)\n",
    "\n",
    "def pbias(sim, obs):\n",
    "    return 100 * np.sum(sim - obs) / np.sum(obs)\n",
    "\n",
    "# ðŸ”¹ Reconstruction des sÃ©ries temporelles\n",
    "y_test_reconstructed = np.concatenate([y_test_rescaled[i] for i in range(y_test_rescaled.shape[0] - 1)], axis=0)\n",
    "y_pred_reconstructed = np.concatenate([y_pred_rescaled[i] for i in range(y_pred_rescaled.shape[0] - 1)], axis=0)\n",
    "\n",
    "# ðŸ”¹ CrÃ©ation du subplot interactif\n",
    "fig = make_subplots(rows=5, cols=2, subplot_titles=[f\"Jour {i+1}\" for i in range(10)])\n",
    "\n",
    "for i in range(10):  # Pour chaque jour de l'horizon de prÃ©diction\n",
    "    y_true = y_test_reconstructed[i::10]\n",
    "    y_pred = y_pred_reconstructed[i::10]\n",
    "    \n",
    "    # Calcul des mÃ©triques\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    correlation = np.corrcoef(y_true, y_pred)[0, 1]\n",
    "    kge_value = kge(y_pred, y_true)\n",
    "    pbias_value = pbias(y_pred, y_true)\n",
    "    \n",
    "    # Ajouter la courbe des valeurs rÃ©elles\n",
    "    fig.add_trace(go.Scatter(y=y_true, mode='lines', name='Y_test (rÃ©el)', line=dict(color='blue')),\n",
    "                  row=(i//2)+1, col=(i%2)+1)\n",
    "    \n",
    "    # Ajouter la courbe des valeurs prÃ©dites\n",
    "    fig.add_trace(go.Scatter(y=y_pred, mode='lines', name='Y_pred (prÃ©dit)', line=dict(color='red', dash='dash')),\n",
    "                  row=(i//2)+1, col=(i%2)+1)\n",
    "    \n",
    "    # Ajouter les mÃ©triques dans le titre du sous-graphe\n",
    "    fig.update_annotations(\n",
    "        selector=dict(text=f\"Jour {i+1}\"),\n",
    "        text=f\"Jour {i+1} \\nRMSE={rmse:.2f}, Corr={correlation:.2f}, KGE={kge_value:.2f}, PBIAS={pbias_value:.2f}%\"\n",
    "    )\n",
    "\n",
    "# Configuration de la mise en page\n",
    "title_text = \"Comparaison des valeurs rÃ©elles et prÃ©dites pour chaque jour de l'horizon de 10 jours\"\n",
    "fig.update_layout(height=900, width=1200, title_text=title_text, showlegend=False)\n",
    "\n",
    "# Affichage interactif\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# ðŸ”¹ Fonction pour calculer le Kling-Gupta Efficiency (KGE)\n",
    "def kling_gupta_efficiency(y_true, y_pred):\n",
    "    mean_obs = np.mean(y_true)\n",
    "    mean_pred = np.mean(y_pred)\n",
    "    \n",
    "    r = np.corrcoef(y_true, y_pred)[0, 1]  # CorrÃ©lation\n",
    "    beta = mean_pred / mean_obs  # Biais moyen\n",
    "    alpha = (np.std(y_pred) / mean_pred) / (np.std(y_true) / mean_obs)  # Ratio des Ã©carts-types\n",
    "    \n",
    "    return 1 - np.sqrt((r - 1) ** 2 + (beta - 1) ** 2 + (alpha - 1) ** 2)\n",
    "\n",
    "# ðŸ”¹ Fonction pour calculer le PBIAS\n",
    "def pbias(y_true, y_pred):\n",
    "    return 100 * np.sum(y_pred - y_true) / np.sum(y_true)\n",
    "\n",
    "# ðŸ”¹ Reconstruction des sÃ©ries temporelles\n",
    "y_test_reconstructed = np.concatenate([y_test_rescaled[i] for i in range(y_test_rescaled.shape[0] - 1)], axis=0)\n",
    "y_pred_reconstructed = np.concatenate([y_pred_rescaled[i] for i in range(y_pred_rescaled.shape[0] - 1)], axis=0)\n",
    "\n",
    "# ðŸ”¹ CrÃ©ation des graphiques\n",
    "fig, axes = plt.subplots(5, 2, figsize=(14, 10), sharex=True, sharey=True)\n",
    "axes = axes.flatten()\n",
    "\n",
    "for i in range(prediction_size):  # Pour chaque jour de l'horizon de prÃ©diction\n",
    "    # SÃ©lectionner les valeurs correspondantes aux sÃ©quences\n",
    "    y_true = y_test_reconstructed[i::prediction_size]  # Prend les vraies valeurs pour le jour i\n",
    "    y_pred = y_pred_reconstructed[i::prediction_size]  # Prend les prÃ©dictions pour le jour i\n",
    "    \n",
    "    # Calcul des mÃ©triques pour le jour i\n",
    "    rmse = np.sqrt(mean_squared_error(y_true, y_pred))\n",
    "    correlation = np.corrcoef(y_true, y_pred)[0, 1]  # Coefficient de corrÃ©lation\n",
    "    kge = kling_gupta_efficiency(y_true, y_pred)  # KGE\n",
    "    pbias_value = pbias(y_true, y_pred)  # PBIAS\n",
    "\n",
    "    # Tracer les courbes\n",
    "    axes[i].plot(y_true, label=\"Y_test (rÃ©el)\", color='blue')\n",
    "    axes[i].plot(y_pred, label=\"Y_pred (prÃ©dit)\", color='red', linestyle='dashed')\n",
    "\n",
    "    # Ajouter la mÃ©trique dans le titre\n",
    "    axes[i].set_title(f\"Jour {i+1}\\nRMSE={rmse:.2f}, Corr={correlation:.2f}, KGE={kge:.2f}, PBIAS={pbias_value:.2f}%\")\n",
    "    \n",
    "    axes[i].grid()\n",
    "    axes[i].legend()\n",
    "\n",
    "plt.suptitle(\"Comparaison des valeurs rÃ©elles et prÃ©dites pour chaque jour de l'horizon de 10 jours\", fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "# ðŸ”¹ Chemin du rÃ©pertoire contenant les fichiers .txt\n",
    "\n",
    "repertoire =\"/home/mgning/work/hyfaa/work_configurations/senegal/test_oualia_cs=1_wm=800_b=0.30/\"\n",
    "# ðŸ”¹ Liste des fichiers .txt dans le rÃ©pertoire\n",
    "fichiers = [f for f in os.listdir(repertoire) if f.endswith('.txt')]\n",
    "# ðŸ”¹ CrÃ©ation d'un DataFrame vide pour fusionner les donnÃ©es\n",
    "df_final = pd.DataFrame()\n",
    "\n",
    "# ðŸ”¹ Boucle sur tous les fichiers .txt\n",
    "for fichier in fichiers:\n",
    "    chemin_fichier = os.path.join(repertoire, fichier)\n",
    "    \n",
    "    # Lire le fichier en tant que DataFrame\n",
    "    df = pd.read_csv(chemin_fichier, sep=\";\")  # Modifier 'sep' si nÃ©cessaire\n",
    "    \n",
    "    # VÃ©rifier si la colonne \"value\" existe\n",
    "    if \"value\" in df.columns:\n",
    "        # Renommer la colonne \"value\" avec le nom du fichier (sans extension)\n",
    "        nom_colonne = os.path.splitext(fichier)[0]\n",
    "        df = df[[\"value\"]].rename(columns={\"value\": nom_colonne})\n",
    "        \n",
    "        # Fusionner les DataFrames (concatÃ©nation horizontale)\n",
    "        if df_final.empty:\n",
    "            df_final = df\n",
    "        else:\n",
    "            df_final = pd.concat([df_final, df], axis=1)\n",
    "\n",
    "# ðŸ”¹ Affichage du DataFrame final\n",
    "print(df_final.head())\n",
    "\n",
    "# ðŸ”¹ Sauvegarde en CSV si besoin\n",
    "df_final.to_csv(\"resultat_final.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import LSTM, Dense\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.metrics import mean_squared_error\n",
    "\n",
    "# Charger les donnÃ©es\n",
    "df = df_final.copy()  # Assurez-vous que df_final contient les bonnes donnÃ©es\n",
    "\n",
    "# SÃ©lection des colonnes\n",
    "features_columns = [\"Q_MANANTALI_AVAL\", \"Q_GUIERS\", \"Q_GHORFA\", \"Q_KABATE\", \"Q_BAFING_MAKANA\"]\n",
    "targets_columns = [\"Q_MANANTALI_AVAL\", \"Q_GUIERS\", \"Q_GHORFA\", \"Q_KABATE\", \"Q_BAFING_MAKANA\"]\n",
    "\n",
    "# Normalisation des donnÃ©es\n",
    "scaler = MinMaxScaler()\n",
    "df_scaled = pd.DataFrame(scaler.fit_transform(df[features_columns]), columns=features_columns, index=df.index)\n",
    "\n",
    "# Fonction pour crÃ©er les sÃ©quences de donnÃ©es\n",
    "def create_sequences(data, target_columns, window_size, prediction_size):\n",
    "    X, y = [], []\n",
    "    for i in range(len(data) - window_size - prediction_size):\n",
    "        X.append(data.iloc[i:i+window_size].values)\n",
    "        y.append(data.iloc[i+window_size:i+window_size+prediction_size][target_columns].values)\n",
    "    return np.array(X), np.array(y)\n",
    "\n",
    "# ParamÃ¨tres\n",
    "window_size = 10  # Nombre de jours utilisÃ©s pour la prÃ©diction\n",
    "prediction_size = 10  # Nombre de jours Ã  prÃ©dire\n",
    "\n",
    "# CrÃ©ation des jeux d'entraÃ®nement et de test\n",
    "train_size = int(len(df_scaled) * 0.3)\n",
    "train_data = df_scaled.iloc[:train_size]\n",
    "test_data = df_scaled.iloc[train_size:]\n",
    "\n",
    "X_train, y_train = create_sequences(train_data, targets_columns, window_size, prediction_size)\n",
    "X_test, y_test = create_sequences(test_data, targets_columns, window_size, prediction_size)\n",
    "\n",
    "# Construction du modÃ¨le LSTM\n",
    "model = Sequential([\n",
    "    LSTM(100, activation='relu', return_sequences=True, input_shape=(window_size, len(features_columns))),\n",
    "    LSTM(50, activation='relu'),\n",
    "    Dense(len(targets_columns) * prediction_size)\n",
    "])\n",
    "\n",
    "model.compile(optimizer='adam', loss='mse')\n",
    "\n",
    "# EntraÃ®nement du modÃ¨le\n",
    "history = model.fit(X_train, y_train.reshape(y_train.shape[0], -1), epochs=50, batch_size=32, validation_data=(X_test, y_test.reshape(y_test.shape[0], -1)))\n",
    "\n",
    "# PrÃ©diction\n",
    "y_pred = model.predict(X_test)\n",
    "y_pred = y_pred.reshape(y_test.shape)  # Reshape pour correspondre aux dimensions originales\n",
    "\n",
    "# Inverser la normalisation\n",
    "y_test_rescaled = scaler.inverse_transform(y_test.reshape(-1, len(targets_columns))).reshape(y_test.shape)\n",
    "y_pred_rescaled = scaler.inverse_transform(y_pred.reshape(-1, len(targets_columns))).reshape(y_pred.shape)\n",
    "\n",
    "# ðŸ“Š Visualisation des prÃ©dictions vs valeurs rÃ©elles\n",
    "fig, axes = plt.subplots(5, 1, figsize=(12, 15), sharex=True)\n",
    "\n",
    "for i, col in enumerate(targets_columns):\n",
    "    axes[i].plot(y_test_rescaled[:, :, i].flatten(), label=\"RÃ©el\", color='blue')\n",
    "    axes[i].plot(y_pred_rescaled[:, :, i].flatten(), label=\"PrÃ©dit\", color='red', linestyle='dashed')\n",
    "    axes[i].set_title(f\"PrÃ©diction de {col}\")\n",
    "    axes[i].legend()\n",
    "    axes[i].grid()\n",
    "\n",
    "plt.xlabel(\"Temps\")\n",
    "plt.suptitle(\"Comparaison des valeurs rÃ©elles et prÃ©dites par LSTM\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43",
   "metadata": {},
   "source": [
    "## Learning rate finder development"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lr_find(self, start_lr=1e-7, end_lr=10, num_iter=100, step_mode=\"exp\", show_plot=True):\n",
    "        \"\"\"Find a good learning rate by training with exponentially growing lr\n",
    "            source: https://github.com/fastai/fastai1/blob/master/fastai/train.py#L33\n",
    "\n",
    "        \n",
    "        Args:\n",
    "            start_lr (float): Starting learning rate\n",
    "            end_lr (float): Maximum learning rate\n",
    "            num_iter (int): Number of iterations to run\n",
    "            step_mode (str): \"exp\" for exponential increase, \"linear\" for linear increase\n",
    "            show_plot (bool): Whether to display the loss plot\n",
    "            \n",
    "        Returns:\n",
    "            tuple: (optimal_lr, learning_rates, losses)\n",
    "        \"\"\"\n",
    "        # Save the original model state\n",
    "        original_state = {\n",
    "            'model': self.model.state_dict(),\n",
    "            'optimizer': self.optimizer\n",
    "        }\n",
    "        \n",
    "        # Initialize optimizer with start_lr\n",
    "        optimizer = self.optimizer(self.model.parameters(), lr=start_lr)\n",
    "        \n",
    "        # Calculate the multiplication factor for each step\n",
    "        if step_mode == \"exp\":\n",
    "            gamma = (end_lr / start_lr) ** (1 / num_iter)\n",
    "        else:\n",
    "            gamma = (end_lr - start_lr) / num_iter\n",
    "            \n",
    "        scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma) if step_mode == \"exp\" else None\n",
    "        \n",
    "        learning_rates = []\n",
    "        losses = []\n",
    "        best_loss = float('inf')\n",
    "        \n",
    "        # Create iterator for training data\n",
    "        iterator = iter(self.train_loader)\n",
    "        \n",
    "        for iteration in range(num_iter):\n",
    "            try:\n",
    "                batch_X, batch_y = next(iterator)\n",
    "            except StopIteration:\n",
    "                iterator = iter(self.train_loader)\n",
    "                batch_X, batch_y = next(iterator)\n",
    "                \n",
    "            # Forward pass\n",
    "            self.model.train()\n",
    "            optimizer.zero_grad()\n",
    "            outputs = self.model(batch_X)\n",
    "            loss = self.criterion(outputs, batch_y.squeeze())\n",
    "            \n",
    "            # Backward pass\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            # Store the values\n",
    "            current_lr = optimizer.param_groups[0]['lr']\n",
    "            learning_rates.append(current_lr)\n",
    "            losses.append(loss.item())\n",
    "            \n",
    "            # Update learning rate\n",
    "            if step_mode == \"exp\":\n",
    "                scheduler.step()\n",
    "            else:\n",
    "                for param_group in optimizer.param_groups:\n",
    "                    param_group['lr'] = start_lr + (gamma * (iteration + 1))\n",
    "            \n",
    "            # Stop if the loss is exploding\n",
    "            if iteration > 0 and losses[-1] > 4 * best_loss:\n",
    "                break\n",
    "                \n",
    "            if losses[-1] < best_loss:\n",
    "                best_loss = losses[-1]\n",
    "        \n",
    "        # Restore the original model state\n",
    "        self.model.load_state_dict(original_state['model'])\n",
    "        \n",
    "        if show_plot:\n",
    "            plt.figure(figsize=(10, 6))\n",
    "            plt.plot(learning_rates, losses)\n",
    "            plt.xscale('log')\n",
    "            plt.xlabel('Learning Rate (log scale)')\n",
    "            plt.ylabel('Loss')\n",
    "            plt.title('Learning Rate Finder')\n",
    "            plt.show()\n",
    "            \n",
    "        # Find the point of steepest descent\n",
    "        smoothed_losses = np.array(losses)\n",
    "        min_grad_idx = np.gradient(smoothed_losses).argmin()\n",
    "        optimal_lr = learning_rates[min_grad_idx]\n",
    "            \n",
    "        return optimal_lr, learning_rates, losses"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45",
   "metadata": {},
   "source": [
    "## Initial Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46",
   "metadata": {},
   "outputs": [],
   "source": [
    "data[\"delta_debit_mgb\"] = data[\"dÃ©bit_mgb\"].diff().fillna(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# CrÃ©ation de la figure\n",
    "fig = go.Figure()\n",
    "\n",
    "# Ajout de la courbe pour P_cumul_7j (Axe principal Y1)\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=data.index, \n",
    "    y=data['dÃ©bit_insitu'], \n",
    "    mode='lines', \n",
    "    name=\"Pluie cumulÃ©e 7j\", \n",
    "    line=dict(color='blue'),\n",
    "    yaxis=\"y1\"  # SpÃ©cifier l'axe Y1\n",
    "))\n",
    "\n",
    "# Ajout de la courbe pour dÃ©bit_insitu (Axe secondaire Y2)\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=data.index, \n",
    "    y=data[\"dÃ©bit_mgb\"], \n",
    "    mode='lines', \n",
    "    name=\"DÃ©bit in situ\", \n",
    "    line=dict(color='red', dash='dash'),\n",
    "    yaxis=\"y2\"  # SpÃ©cifier l'axe Y2\n",
    "))\n",
    "\n",
    "# Mise en forme du graphique avec deux axes Y\n",
    "fig.update_layout(\n",
    "    title=\"Ã‰volution de P_cumul_7j et DÃ©bit in situ en fonction du temps\",\n",
    "    xaxis=dict(title=\"Temps\"),\n",
    "    yaxis=dict(\n",
    "        title=\"Pluie cumulÃ©e 7j (mm)\", \n",
    "        #titlefont=dict(color=\"blue\"), \n",
    "        tickfont=dict(color=\"blue\"),\n",
    "        side=\"left\"\n",
    "    ),\n",
    "    yaxis2=dict(\n",
    "        title=\"DÃ©bit in situ (mÂ³/s)\", \n",
    "        #titlefont=dict(color=\"red\"), \n",
    "        tickfont=dict(color=\"red\"),\n",
    "        overlaying=\"y\",  # Superposition sur l'axe principal\n",
    "        side=\"right\"  # Placement Ã  droite\n",
    "    ),\n",
    "    legend=dict(x=0.02, y=0.98),  # Position de la lÃ©gende\n",
    "    template=\"plotly_white\"\n",
    ")\n",
    "\n",
    "# Affichage\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48",
   "metadata": {},
   "outputs": [],
   "source": [
    "import plotly.graph_objects as go\n",
    "\n",
    "# CrÃ©ation de la figure\n",
    "fig = go.Figure()\n",
    "\n",
    "# Ajout de la courbe pour P_cumul_7j (Axe principal Y1)\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=data.index, \n",
    "    y=data['dÃ©bit_insitu'], \n",
    "    mode='lines', \n",
    "    name=\"Pluie cumulÃ©e 7j\", \n",
    "    line=dict(color='blue'),\n",
    "    yaxis=\"y1\"  # SpÃ©cifier l'axe Y1\n",
    "))\n",
    "\n",
    "# Ajout de la courbe pour dÃ©bit_insitu (Axe secondaire Y2)\n",
    "fig.add_trace(go.Scatter(\n",
    "    x=data.index, \n",
    "    y=data[\"delta_debit_mgb\"], \n",
    "    mode='lines', \n",
    "    name=\"DÃ©bit in situ\", \n",
    "    line=dict(color='red', dash='dash'),\n",
    "    yaxis=\"y2\"  # SpÃ©cifier l'axe Y2\n",
    "))\n",
    "\n",
    "# Mise en forme du graphique avec deux axes Y\n",
    "fig.update_layout(\n",
    "    title=\"Ã‰volution de P_cumul_7j et DÃ©bit in situ en fonction du temps\",\n",
    "    xaxis=dict(title=\"Temps\"),\n",
    "    \n",
    "    yaxis=dict(\n",
    "        title=\"Pluie cumulÃ©e 7j (mm)\", \n",
    "        # titlefont=dict(color=\"blue\"), \n",
    "        tickfont=dict(color=\"blue\"),\n",
    "        side=\"left\"\n",
    "    ),\n",
    "    \n",
    "    yaxis2=dict(\n",
    "        title=\"DÃ©bit in situ (mÂ³/s)\", \n",
    "        # titlefont=dict(color=\"red\"), \n",
    "        tickfont=dict(color=\"red\"),\n",
    "        overlaying=\"y\",  # Superposition sur l'axe principal\n",
    "        side=\"right\"  # Placement Ã  droite\n",
    "    ),\n",
    "\n",
    "    legend=dict(x=0.02, y=0.98),  # Position de la lÃ©gende\n",
    "    template=\"plotly_white\"\n",
    ")\n",
    "\n",
    "# Affichage\n",
    "fig.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
